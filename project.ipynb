{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/DL-Project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Domain Adaptation\n",
        "In this project, our goal is to achieve an improvement in accuracy with respect to a baseline. The latter simply consists in a pre-trained ResNet34 fine-tuned supervisedly on the source training dataset. The baseline accuracy percentage has been obtained testing on the target domain test set.  \n",
        "It is worth to note that when we talk about source and target datasets we refers to the whole dataset where real world and product images resides.  \n",
        "For both the aforesaid domains, we used an $80\\%$/$20\\%$ ratio for the split into training and target sets respectively.  \n",
        "For the DA implementation, we decided to deploy the approach proposed in [SymNet](https://arxiv.org/pdf/1904.04663.pdf) which presents a quite simple architecture structure but it reasons more on losses definitions level.  \n",
        "Long story short, this paper is inspyred by the theory of *injecting confusion to enforce the feature extractor to learn invariant features* with respect to the domain shift. Thoose features are learned via domain-adversarial training.    \n",
        "Concerning the structure, the ResNet34's one is modified cutting of the classifier `nn.Sequential()` block and appending two classifiers :\n",
        "- $C_s$ : source classifier;\n",
        "- $C_t$ : target classifier.\n",
        "\n",
        "As it is possible to infer from the network name, everything evolves around building a symmetric design of source and target task classfiers from which another classifier ($C_{st}$) is built on top. The latter shares its layer neurons with $C_s$ and $C_t$.  \n",
        "Both domain discrimination and domain confusion are implemented based on the constructed additional classifier."
      ],
      "metadata": {
        "id": "hYWXadIfJkAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # to mount personal drive\n",
        "\n",
        "\n",
        "from tqdm import tqdm   # for progress bar \n",
        "from time import sleep\n",
        "\n",
        "import torch  # importing pytorch\n",
        "import torch.optim as optim  # importing optimizer module\n",
        "from torch.utils.data import Subset  # useful in defining data of interest in a dataset\n",
        "import torch.nn as nn  # Neural Network tools\n",
        "from torch.utils.tensorboard import SummaryWriter # to get plots of trends\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T  # to apply transformations to dataset images\n",
        "from torchvision.datasets import ImageFolder  # to load and applying transformations on data\n",
        "#import torchvision.transforms.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # to split a dataset into training and test set\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XDSUxCL6Jwj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "WVUMK-gw8w8V",
        "outputId": "1e4dee7a-b2be-420d-d2d3-fae3c41113ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function for extracting data we'll be working with\n",
        "In `get_data(batch_size, root_dir)` the following steps are performed :\n",
        "- images transforms are defined. In particular, the adopted transformation sequence has been found here [ResNet Transforms](https://pytorch.org/hub/pytorch_vision_resnet/);\n",
        "- images form the local drive are loaded and the transforms applied;\n",
        "- data splitting;\n",
        "- collating individual fetched data samples into batches.\n",
        "The returned objects are the real world and product domain loaders. "
      ],
      "metadata": {
        "id": "z4uiw_OkK6tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_dataset(dataset, n=6):\n",
        "  img = np.vstack((np.hstack((np.asarray(dataset[i][0]) for _ in range(n)))\n",
        "                   for i in range(len(dataset))))\n",
        "  plt.imshow(img)\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "7fAVq2JlK8BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, root_dir):\n",
        "  \"\"\"\n",
        "\n",
        "  Params:\n",
        "  ------\n",
        "  root_dir: str\n",
        "    Directory of adaptiope_small (e.g. \"something/something_else/adaptiope_small\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Transforms for resnet found there https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "  transform_img = list()\n",
        "  transform_img.append(T.Resize(256))\n",
        "  transform_img.append(T.CenterCrop(224))\n",
        "  transform_img.append(T.ToTensor())\n",
        "  transform_img.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "  transform_img = T.Compose(transform_img)\n",
        "\n",
        "  # load data\n",
        "  product_images_dataset = ImageFolder(root = f\"{root_dir}/product_images/\", transform = transform_img)\n",
        "  rw_images_dataset = ImageFolder(root = f\"{root_dir}/real_life/\", transform = transform_img)\n",
        "\n",
        "  product_train_indexes, product_test_indexes = train_test_split(list(range(len(product_images_dataset.targets))),\n",
        "                                                test_size = 0.2, stratify = product_images_dataset.targets, random_state = 42)\n",
        "  \n",
        "  rw_train_indexes, rw_test_indexes = train_test_split(list(range(len(rw_images_dataset.targets))),\n",
        "                                                test_size = 0.2, stratify = rw_images_dataset.targets, random_state = 42)\n",
        "  \n",
        "\n",
        "  product_train_data = Subset(product_images_dataset, product_train_indexes)\n",
        "  product_test_data = Subset(product_images_dataset, product_test_indexes)\n",
        "\n",
        "  rw_train_data = Subset(rw_images_dataset, rw_train_indexes)\n",
        "  rw_test_data = Subset(rw_images_dataset, rw_test_indexes)\n",
        "\n",
        "  product_train_loader = torch.utils.data.DataLoader(product_train_data, batch_size, shuffle = False)\n",
        "  product_test_loader = torch.utils.data.DataLoader(product_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  rw_train_loader = torch.utils.data.DataLoader(rw_train_data, batch_size, shuffle = False)\n",
        "  rw_test_loader = torch.utils.data.DataLoader(rw_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  return product_train_loader, product_test_loader, rw_train_loader, rw_test_loader\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wXlIpqRk8xZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network initialization\n",
        "The ResNet34 pretrained model is intialized. Since for the baseline we decided to perform a simple fine-tune, the original classifier layer has been overwritten and the gradients has been enabled."
      ],
      "metadata": {
        "id": "Q70XX70ALYR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_resnet34(num_classes, pretrained = True):\n",
        "\n",
        "  model = torchvision.models.resnet34(pretrained=pretrained)\n",
        "\n",
        "  in_features = model.fc.in_features\n",
        "\n",
        "  ##model.fc = nn.Sequential(nn.Linear(512, num_classes))#, nn.LogSoftmax(dim = 1))\n",
        "  model.fc = nn.Linear(512, num_classes)\n",
        "  for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Xq4k2RGV81M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-entropy loss for training data\n",
        "The following very simple function returns the computed cross-entropy loss value."
      ],
      "metadata": {
        "id": "takAN3EMLcj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for labelled data\n",
        "def get_ce_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "F-mp8CmM83cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training procedure\n",
        "The `training_step_baseline` function requires the following parameters :\n",
        "- *net* : network model. This is needed to obtain the predicted classifications results. The latter are used to compute the cross-entropy loss;\n",
        "- *data_loader* : to acces the training data;\n",
        "- *optimizer* : the optimizer instance to perform an optimization step after having computed the CE loss;\n",
        "- *cost_function* : function that computes a specific loss function. In this case, the classic cross-entropy is used;\n",
        "- *device* : supposed to be a GPU to work with tensor. From a general point of view this is the device where computaitons take place.\n",
        "\n",
        "#### What does happen in the function?\n",
        "Briefly, the net is initialized to prepare it to the training procedure.  \n",
        "The training dataset is iteratively cycled through on groups of `batch_size` dimension. For each sample in the current batch, inputs and targets are moved to the specified device, the predicted outputs are computed and the losses computed.  \n",
        "After that, an optimization step is performed."
      ],
      "metadata": {
        "id": "lBt70fF-LlWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_baseline(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  net.train() \n",
        " \n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "\n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ],
      "metadata": {
        "id": "DMiSWtTz87wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test procedure\n",
        "The network is set to evaluation mode.  \n",
        "After this, we disable all the gradients in order to avoid changing some weights. The tesiting procedure from now on is pretty much analogous to what's been done during the training with the only difference that an optimization step is not performed here."
      ],
      "metadata": {
        "id": "IdP1pK-LLrlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      \n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "9bHErRLpLuEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function (Product → Real World) \n",
        "This function is meant to be a 'wrapping up' function where every aforecited function is called when needed.  \n",
        "First, the parameters values are defined as arguments of the function.  \n",
        "Then, sequentially :     \n",
        "- extract, process and load data;\n",
        "- network, optimizer and cost function initialization;\n",
        "- iterating a certain number of times equal to a fixed number of epochs. In here the following steps are performed :    \n",
        "  - computation of training loss and accuracy;\n",
        "  - computation of test loss and accuracy;\n",
        "  - informing the writer of the got values.\n",
        "\n",
        "- At the end of the training, the network is tested on both test sets of source and tagret domains."
      ],
      "metadata": {
        "id": "IiuMV7H_MUcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_PRD_to_RW(batch_size=128, \n",
        "         device='cuda', \n",
        "         learning_rate=0.0001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = 20, \n",
        "         img_root=\"gdrive/My Drive/Colab Notebooks/data/adaptiope_small\",\n",
        "         runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "         ):\n",
        "\n",
        "  writer = SummaryWriter(log_dir=runs_dir)\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  #optimizer = get_optimizer_SGD(net, learning_rate, wd = weight_decay, momentum = momentum)\n",
        "  optimizer = ResNetOptimizer(net, epochs)\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  print('Time to train!\\n==========================BASELINE========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    ##BASELINE\n",
        "\n",
        "\n",
        "    # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "    train_loss, train_accuracy = training_step_baseline(net, source_train_loader, optimizer, cost_function, device)\n",
        "    #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "    test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "    optimizer.update_lr()\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # add values to logger\n",
        "    \"\"\"writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\"\"\"\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, source_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "JDFkQoLq8-oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_PRD_to_RW()"
      ],
      "metadata": {
        "id": "DGp93KtW9fDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/baseline/PRD2RW\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "E83UwVoYM0G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function (Real World → Product)"
      ],
      "metadata": {
        "id": "UMt4Il3QMf3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_RW_to_PRD(batch_size=128, \n",
        "         device='cuda', \n",
        "         learning_rate=0.0001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = 20, \n",
        "         img_root=\"gdrive/My Drive/Colab Notebooks/data/adaptiope_small\",\n",
        "         runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "         ):\n",
        "\n",
        "  writer = SummaryWriter(log_dir=runs_dir)\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  target_train_loader, target_test_loader, source_train_loader, source_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  #optimizer = get_optimizer_SGD(net, learning_rate, wd = weight_decay, momentum = momentum)\n",
        "  optimizer = ResNetOptimizer(net, epochs)\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  print('Time to train!\\n==========================BASELINE========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    ##BASELINE\n",
        "\n",
        "\n",
        "    # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "    train_loss, train_accuracy = training_step_baseline(net, source_train_loader, optimizer, cost_function, device)\n",
        "    #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "    test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "    optimizer.update_lr()\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # add values to logger\n",
        "    \"\"\"writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\"\"\"\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, source_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "A5zN77uaMgsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_RW_to_PRD()"
      ],
      "metadata": {
        "id": "j7IYQVTJNrfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/baseline/RW2PRD\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "cock6OcuNslg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UPPER BOUND IMPLEMENTATION\n",
        "The *upper bound* consists in training supervisedly using the target domain's labels and testing on the target domain itself."
      ],
      "metadata": {
        "id": "St6XNk3SN74g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Product $\\to$ Real World"
      ],
      "metadata": {
        "id": "c4aM8n3eOENz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_upper_bound(batch_size=BATCH_SIZE, \n",
        "                     device=cuda, \n",
        "                     learning_rate=0.0001, \n",
        "                     weight_decay=0.000001,\n",
        "                     momentum=0.9, \n",
        "                     epochs=15,\n",
        "                     entropy_loss_weight=0.1,\n",
        "                     nr_classes = num_classes, \n",
        "                     img_root=rootdir_matteo,\n",
        "                     runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "                     ):\n",
        "  \n",
        "  writer = SummaryWriter(log_dir=f\"{runsdir_matteo}/runs_upper_bound/PRD2RW\")\n",
        "\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = ResNetOptimizer()\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  \n",
        "  print('Time to train!\\n==========================UPPER BOUND========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    # Inspyred by : https://towardsdatascience.com/training-models-with-a-progress-a-bar-2b664de3e13e\n",
        "    with tqdm(source_train_loader, unit=\"batch\") as tepoch:\n",
        "      for data, target in tepoch:\n",
        "          tepoch.set_description(f\"Epoch {e}\")\n",
        "          # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "          train_loss, train_accuracy = training_step_baseline(net, target_train_loader, optimizer, cost_function, device)\n",
        "          #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "          test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "          tepoch.set_postfix(train_loss=train_loss, training_accuracy = train_accuracy, tst_loss = test_loss, tst_accuracy=test_accuracy)\n",
        "          sleep(0.1)\n",
        "\n",
        "    optimizer.update_lr()\n",
        "\n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, target_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "tCmjRfjGOInh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "knNUV3J-OOLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_upper_bound()"
      ],
      "metadata": {
        "id": "IQHeg9EAOOCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/runs_upper_bound/PRD2RW\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "YDHqcUovON1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real World $\\to$ Product"
      ],
      "metadata": {
        "id": "2n8SQNQcOUdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_UB_RW2PRD(batch_size=BATCH_SIZE, \n",
        "                     device=cuda, \n",
        "                     learning_rate=0.0001, \n",
        "                     weight_decay=0.000001,\n",
        "                     momentum=0.9, \n",
        "                     epochs=15,\n",
        "                     entropy_loss_weight=0.1,\n",
        "                     nr_classes = num_classes, \n",
        "                     img_root=rootdir_matteo,\n",
        "                     runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "                     ):\n",
        "  \n",
        "  writer = SummaryWriter(log_dir=f\"{runsdir_matteo}/runs_upper_bound/RW2PRD\")\n",
        "\n",
        "  target_train_loader, target_test_loader, source_train_loader, source_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = ResNetOptimizer()\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  \n",
        "  print('Time to train!\\n==========================UPPER BOUND========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    # Inspyred by : https://towardsdatascience.com/training-models-with-a-progress-a-bar-2b664de3e13e\n",
        "    with tqdm(source_train_loader, unit=\"batch\") as tepoch:\n",
        "      for data, target in tepoch:\n",
        "          tepoch.set_description(f\"Epoch {e}\")\n",
        "          # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "          train_loss, train_accuracy = training_step_baseline(net, target_train_loader, optimizer, cost_function, device)\n",
        "          #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "          test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "          tepoch.set_postfix(train_loss=train_loss, training_accuracy = train_accuracy, tst_loss = test_loss, tst_accuracy=test_accuracy)\n",
        "          sleep(0.1)\n",
        "    \n",
        "    optimizer.update_lr()\n",
        "    \n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, target_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "Z748Ld_vOVnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "N7L6yQgiQG8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_UB_RW2PRD()"
      ],
      "metadata": {
        "id": "cDvEL0V4QIuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/runs_upper_bound/RW2PRD\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "fBfFRAUkQIkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain Adaptation Technique : SymNet\n"
      ],
      "metadata": {
        "id": "qtwL0pU1QMvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SymNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Class representing the proposed symmetric network\n",
        "  \"\"\"\n",
        "  def __init__(self, n_classes: int = 20) -> None:\n",
        "    super(SymNet, self).__init__()\n",
        "    # Taking the feature extractor of resnet34\n",
        "    # Reference: https://stackoverflow.com/questions/55083642/extract-features-from-last-hidden-layer-pytorch-resnet18\n",
        "    resnet = initialize_resnet34(20, True)\n",
        "    self.feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "    # print(self.feature_extractor)\n",
        "    self.source_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "    self.target_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "  \n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> tuple:\n",
        "    features = self.feature_extractor(x)\n",
        "    features = features.squeeze()\n",
        "    source_output = self.source_classifier(features)\n",
        "    # source_output = nn.Softmax(source_output)\n",
        "\n",
        "    target_output = self.target_classifier(features)\n",
        "    # target_output = nn.Softmax(target_output)\n",
        "\n",
        "    source_target_classifier = torch.cat((source_output, target_output), dim=1)\n",
        "    \n",
        "    return source_output , target_output, source_target_classifier\n",
        "  \n",
        "  def parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Defines an iterator over all the paramters of the netowork\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Network parameter\n",
        "    \"\"\"\n",
        "    fe = list(self.feature_extractor.parameters())\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = fe + sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n",
        "    \n",
        "  def classifier_parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Defines an iterator over parameters of the classification layer\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Classification layer parameter\n",
        "    \"\"\"\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n",
        "\n",
        "  def feature_extractor_parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Defines an iterator over parameters of the feature extractor\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Feature extractor parameter\n",
        "    \"\"\"\n",
        "    return self.feature_extractor.parameters()\n"
      ],
      "metadata": {
        "id": "jw9TzdJA8--0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class AnnealingOptimizer(torch.optim.Optimizer, ABC):\n",
        "  \"\"\"\n",
        "  Defines and abstract class in order to implement an sgd optimizer using an annealing strategy\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0) -> None:\n",
        "    if not 0.0 <= lr:\n",
        "      raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "    if not 0 <= epoch:\n",
        "      raise ValueError(f\"Invalid epoch value: {epoch}\")\n",
        "    \n",
        "    self.nr_epochs = nr_epochs\n",
        "    self.epoch = epoch\n",
        "    self._alpha = 10\n",
        "    self._beta = 0.75\n",
        "    self._base_lr = lr\n",
        "\n",
        "  def update_lr(self):\n",
        "    \"\"\"\n",
        "    Updates the learning rate using the annealing strategy.\n",
        "    In order to let the annealing strategy to work correctly, this method should be called at every epoch during the network training\n",
        "    \"\"\"\n",
        "    self.epoch += 1\n",
        "    new_lr = self._compute_lr()\n",
        "    for g in self.optimizer.param_groups:\n",
        "      if g[\"name\"] == \"fe\":\n",
        "        g[\"lr\"] = new_lr\n",
        "      else:\n",
        "        g[\"lr\"] = new_lr*10\n",
        "\n",
        "    \n",
        "  def _compute_lr(self):\n",
        "    \"\"\"\n",
        "    Computes the learning rate using the proposed annealing strategy\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      updated learning rate\n",
        "    \"\"\"\n",
        "    etap = 1 / ((1 + self._alpha * self.epoch / self.nr_epochs ) ** self._beta)\n",
        "    return self._base_lr * etap\n",
        "\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    self.optimizer.zero_grad()\n",
        "  \n",
        "  \n",
        "\n",
        "class ResNetOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for Resnet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0) -> None:\n",
        "    super(ResNetOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "    \n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': self.__get_fe_params(model), \"name\": \"fe\"},\n",
        "                {'params': model.fc.parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=0.9)\n",
        "    \n",
        "\n",
        "  def __get_fe_params(self, model):\n",
        "    \"\"\"\n",
        "    Takes feature extractor parameters for ResNet\n",
        "    \"\"\"\n",
        "    fe_layers = list(model.children())[:-1]\n",
        "    all_parameters = [param for layer in fe_layers for param in layer.parameters()]\n",
        "    for param in all_parameters:\n",
        "      yield param\n",
        "\n",
        "\n",
        "\n",
        "class SymNetOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for SymNet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0):\n",
        "    super(SymNetOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': model.feature_extractor_parameters(), \"name\": \"fe\"},\n",
        "                {'params': model.classifier_parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=0.9)\n",
        "    # self.optimizer = optim.SGD(model.parameters(), lr=self.__compute_lr(), momentum = .9)\n",
        "  \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "s0ZPPxfrXsuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def source_loss(output, label):\n",
        "  \"\"\"\n",
        "  Returns\n",
        "  -------\n",
        "  Cross entropy loss\n",
        "  \"\"\"\n",
        "  loss_fun = nn.CrossEntropyLoss()\n",
        "  loss = loss_fun(output, label)\n",
        "  return loss\n",
        "\n",
        "def target_loss(output, label):\n",
        "  return source_loss(output, label)\n",
        "\n",
        "def source_target_loss(output, st = True):\n",
        "  \"\"\"\n",
        "  st = True if train sample belongs to source, False otherwise\n",
        "  \"\"\"\n",
        "  n_classes = int(output.size(1)/2)\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output)\n",
        "  if st:\n",
        "    loss = -(prob_out[:,:n_classes].sum(1).log().mean())\n",
        "  else:\n",
        "    loss = -(prob_out[:,n_classes:].sum(1).log().mean())\n",
        "  return loss\n",
        "\n",
        "def feature_category_loss(output_st, label):\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  loss_fun_1 = nn.CrossEntropyLoss()\n",
        "  loss_fun_2 = nn.CrossEntropyLoss()\n",
        "\n",
        "  loss_1 = loss_fun_1(output_st[:, :n_classes], label)/2\n",
        "  loss_2 = loss_fun_2(output_st[:,n_classes:], label)/2\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "def feature_domain_loss(output_st):\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output_st)\n",
        "\n",
        "  loss_1 = -(prob_out[:,:n_classes]).sum(1).log().mean()/2\n",
        "  loss_2 = -(prob_out[:,n_classes:]).sum(1).log().mean()/2\n",
        "\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "\n",
        "\n",
        "def entropyMinimizationPrinciple(output_st):\n",
        "    nr_classes = int(output_st.size(1)/2)\n",
        "    soft = nn.Softmax(dim=1)\n",
        "    prob_out = soft(output_st)\n",
        "\n",
        "    p_st_source = prob_out[:, :nr_classes]\n",
        "    p_st_target = prob_out[:, nr_classes:]\n",
        "    qst = p_st_source + p_st_target\n",
        "\n",
        "    emp = -qst.log().mul(qst).sum(1).mean()\n",
        "\n",
        "    return emp"
      ],
      "metadata": {
        "id": "cSa0gFgR9HEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_uda(net, src_data_loader, target_data_loader, optimizer, lam, e,device = 'cuda'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_classifier_loss = 0.\n",
        "  cumulative_feature_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  target_iter = iter(target_data_loader)\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs_source, labels) in enumerate(src_data_loader):\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    except:\n",
        "      target_iter = iter(target_data_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs_source = inputs_source.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    length_source_input = inputs_source.shape[0]\n",
        "\n",
        "    ## concatenation along batch dimension.\n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "\n",
        "    # forward pass\n",
        "    c_s, c_t, c_st = net(inputs)\n",
        "\n",
        "    c_s_source = c_s[:length_source_input,:]\n",
        "    c_s_target = c_s[length_source_input:,:]\n",
        "\n",
        "    c_t_source = c_t[:length_source_input,:]\n",
        "    c_t_target = c_t[length_source_input:,:]\n",
        "\n",
        "    c_st_source = c_st[:length_source_input,:]\n",
        "    c_st_target = c_st[length_source_input:,:]\n",
        "\n",
        "\n",
        "    # Equation 5 of the paper\n",
        "    error_source_task = source_loss(c_s_source, labels)\n",
        "\n",
        "    # Equation 6 of the paper\n",
        "    error_target_task = target_loss(c_t_source, labels)\n",
        "\n",
        "    # Equation 7 of the paper\n",
        "    domain_loss_source = source_target_loss(c_st_source)\n",
        "    domain_loss_target = source_target_loss(c_st_target, st = False)\n",
        "    error_domain = domain_loss_source + domain_loss_target\n",
        "\n",
        "    classifier_total_loss = error_source_task + error_target_task + error_domain\n",
        "\n",
        "    classifier_total_loss.backward(retain_graph = True)\n",
        "\n",
        "    for param in net.feature_extractor.parameters():\n",
        "      param.grad.data.zero_()\n",
        "    \n",
        "    class_params = []\n",
        "    for param in net.source_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "    for param in net.target_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "\n",
        "    # Equation 8 of the paper\n",
        "    error_feature_category = feature_category_loss(c_st_source, labels)\n",
        "\n",
        "    # Equation 9 of the paper\n",
        "    error_feature_domain = feature_domain_loss(c_st_target)\n",
        "\n",
        "    min_entropy = entropyMinimizationPrinciple(c_st_target)\n",
        "\n",
        "    # Equations 11 of the paper\n",
        "    feature_total_loss = error_feature_category + lam * (error_feature_domain + min_entropy)\n",
        "\n",
        "    feature_total_loss.backward()\n",
        "\n",
        "    idx = 0\n",
        "    for param in net.source_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "    for param in net.target_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "\n",
        "    \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## optimizer classifier losses composed loss\n",
        "    ## order is important here!\n",
        "    \n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    source_samples+=inputs_source.shape[0]\n",
        "    target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_classifier_loss += classifier_total_loss.item()\n",
        "    cumulative_feature_loss += feature_total_loss.item()\n",
        "    _, predicted = c_s_source.max(dim = 1) ## to get the maximum probability\n",
        "    cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "  return cumulative_classifier_loss/source_samples, cumulative_feature_loss/target_samples, cumulative_accuracy/source_samples*100\n"
      ],
      "metadata": {
        "id": "dGdsgnes9HcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(net, data_target_test_loader, device='cuda:0'):\n",
        "\n",
        "    '''\n",
        "    Params\n",
        "    ------\n",
        "\n",
        "    net : model \n",
        "    data_loader : DataLoader obj of the domain to test on\n",
        "    cost_function : cost function used to address accuracies (not necessary) -> TargetClassifierLoss\n",
        "    device : GPU or CPU device\n",
        "\n",
        "    '''\n",
        "\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(data_target_test_loader):\n",
        "\n",
        "            # load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = labels.to(device)\n",
        "        \n",
        "            # forward pass\n",
        "            _, c_t, _ = net(inputs)\n",
        "\n",
        "            # apply the loss\n",
        "            loss = target_loss(c_t, targets)\n",
        "\n",
        "            # print statistics\n",
        "            samples+=inputs.shape[0]\n",
        "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "            _, predicted = c_t.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "hQiFy0TZ9j3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['backpack', 'bookcase', 'car jack', 'comb', 'crown', 'file cabinet', 'flat iron', 'game controller', 'glasses', 'helicopter', 'ice skates', 'letter tray', 'monitor', 'mug', 'network switch', 'over-ear headphones', 'pen', 'purse', 'stand mixer', 'stroller']\n",
        "\n",
        "cuda = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 128\n",
        "num_classes = len(classes)\n",
        "rootdir_matteo = '/content/gdrive/MyDrive/Colab Notebooks/Deep Learning labs/DA Project/adaptiope_small'\n",
        "rootdir_alessandro = 'gdrive/My Drive/Colab Notebooks/data/adaptiope_small'\n",
        "rootdir_alessandro_uni = 'gdrive/My Drive/project/data/adaptiope_small'"
      ],
      "metadata": {
        "id": "ixVFg1jb9mLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Product $\\to$ Real World"
      ],
      "metadata": {
        "id": "IxD91oKuQg0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "def main_uda(batch_size=128,\n",
        "         device=cuda, \n",
        "         lr = 0.01,\n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=40,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = num_classes, \n",
        "         img_root=rootdir_alessandro\n",
        "         ):\n",
        "    \n",
        "  # writer = SummaryWriter(log_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\")\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = SymNet().to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = SymNetOptimizer(model = net, nr_epochs = epochs) #get_optimizer_ADAM_uda(model=net, e=0, nr_epochs = epochs,lr=lr, wd=weight_decay)\n",
        "  # optimizer_2 = get_optimizer_ADAM_uda(model=net, lr=lr, wd=weight_decay, e=0, nr_epochs=epochs, classifier=False)\n",
        "  print('Got optimizers')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    lam = 2 / (1 + math.exp(-1 * 10 * e / epochs)) - 1\n",
        "    #def training_step_uda(net, src_data_loader, target_data_loader, optimizer_1, optimizer_2, lam, device = 'cuda')\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n",
        "                                                        target_data_loader=target_train_loader, \n",
        "                                                        optimizer=optimizer, lam=lam, e=e, device=device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    test_loss, test_accuracy = test_step(net, target_test_loader, device)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    optimizer.update_lr()"
      ],
      "metadata": {
        "id": "h2-rMG7B9u6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JrQkjYQQQncm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_uda()"
      ],
      "metadata": {
        "id": "sLOVe5Q99xmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 490\n",
        "runs = f\"{runsdir_matteo}/DA/PRD2RW\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "Jy8EPD-l92UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real World $\\to$ Product"
      ],
      "metadata": {
        "id": "5FS6g9wxQwuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_uda_RW2PRD(batch_size=128,\n",
        "         device=cuda, \n",
        "         lr = 0.01,\n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=30,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = num_classes, \n",
        "         img_root=rootdir_matteo\n",
        "         ):\n",
        "    \n",
        "  writer = SummaryWriter(log_dir=f\"{runsdir_matteo}/DA/RW2PRD\")\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  target_train_loader, target_test_loader, source_train_loader, source_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = SymNet().to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = SymNetOptimizer(model = net, nr_epochs = epochs) #get_optimizer_ADAM_uda(model=net, e=0, nr_epochs = epochs,lr=lr, wd=weight_decay)\n",
        "  # optimizer_2 = get_optimizer_ADAM_uda(model=net, lr=lr, wd=weight_decay, e=0, nr_epochs=epochs, classifier=False)\n",
        "  print('Got optimizers')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    lam = 2 / (1 + math.exp(-1 * 10 * e / epochs)) - 1\n",
        "    #def training_step_uda(net, src_data_loader, target_data_loader, optimizer_1, optimizer_2, lam, device = 'cuda')\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n",
        "                                                        target_data_loader=target_train_loader, \n",
        "                                                        optimizer=optimizer, lam=lam, e=e, device=device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    test_loss, test_accuracy = test_step(net, target_test_loader, device)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_ce_loss', train_ce_loss, e + 1)\n",
        "    writer.add_scalar('Loss/train_en_loss', train_en_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "\n",
        "\n",
        "    optimizer.update_lr()\n",
        "  \n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "2WTz1wd0QxHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_5SY9Je0Q1I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_uda_RW2PRD()"
      ],
      "metadata": {
        "id": "jmqwwlefQ3oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}