{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/DL-Project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Conventions and notes:_\n",
        "\n",
        "- _Some small sentences of the paper are copied exactly in this document. This is done in parts when we felt that there were no reason to rewrite, better explain or summarise the concept because it was already clear and concise for us._\n",
        "\n",
        "- _Wherever code is not commented, it is because we thought that it was something trivial_.\n",
        "\n",
        "- _Wherever there is no markdown preceding some chunk of code, it is because it was done in class, a similar chunk of code was already explained earlier or perhaps a docstring was more suitable for the context_\n",
        "\n",
        "# Unsupervised Domain Adaptation\n",
        "In this project, our goal is to achieve an improvement in accuracy with respect to a baseline. The latter simply consists in a pre-trained ResNet34 fine-tuned supervisedly on the source training dataset. The baseline accuracy percentage has been obtained testing on the target domain test set.  \n",
        "It is worth to note that when we talk about source and target datasets we refers to the whole dataset where real world and product images resides.  \n",
        "For both the aforesaid domains, we used an $80\\%$/$20\\%$ ratio for the split into training and target sets respectively.  \n",
        "For the DA implementation, we decided to deploy the approach proposed in [SymNet](https://arxiv.org/pdf/1904.04663.pdf) which presents a quite simple architecture structure but it reasons more on losses definitions level.  \n",
        "Long story short, this paper is inspyred by the theory of *injecting confusion to enforce the feature extractor to learn invariant features* with respect to the domain shift. Thoose features are learned via domain-adversarial training.    \n",
        "Concerning the structure, the ResNet34's one is modified cutting of the classifier `nn.Sequential()` block and appending two classifiers :\n",
        "- $C_s$ : source classifier;\n",
        "- $C_t$ : target classifier.\n",
        "\n",
        "As it is possible to infer from the network name, everything evolves around building a symmetric design of source and target task classfiers from which another classifier ($C_{st}$) is built on top. The latter shares its layer neurons with $C_s$ and $C_t$.  \n",
        "Both domain discrimination and domain confusion are implemented based on the constructed additional classifier."
      ],
      "metadata": {
        "id": "hYWXadIfJkAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # to mount personal drive\n",
        "\n",
        "\n",
        "from tqdm import tqdm   # for progress bar \n",
        "from time import sleep\n",
        "\n",
        "import torch  # importing pytorch\n",
        "import torch.optim as optim  # importing optimizer module\n",
        "from torch.utils.data import Subset  # useful in defining data of interest in a dataset\n",
        "import torch.nn as nn  # Neural Network tools\n",
        "from torch.utils.tensorboard import SummaryWriter # to get plots of trends\n",
        "from torch.utilis.data import DataLoader\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T  # to apply transformations to dataset images\n",
        "from torchvision.datasets import ImageFolder  # to load and applying transformations on data\n",
        "#import torchvision.transforms.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # to split a dataset into training and test set\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "XDSUxCL6Jwj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "WVUMK-gw8w8V",
        "outputId": "1e4dee7a-b2be-420d-d2d3-fae3c41113ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Extraction\n",
        "In `get_data(batch_size, root_dir)` the following steps are performed :\n",
        "- images transforms are defined. In particular, the adopted transformation sequence has been found there: [ResNet Transforms](https://pytorch.org/hub/pytorch_vision_resnet/);\n",
        "- images from the local drive are loaded and the transforms applied;\n",
        "- data splitting;\n",
        "- collating individual fetched data samples into batches.\n",
        "The returned objects are the real world and product domain data loaders."
      ],
      "metadata": {
        "id": "z4uiw_OkK6tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size: int, root_dir: str) -> tuple(torch.utilis.data.DataLoader):\n",
        "  \"\"\"\n",
        "\n",
        "  Params:\n",
        "  ------\n",
        "  batch_size: int\n",
        "    batch size for the dataloader\n",
        "  root_dir: str\n",
        "    Directory of adaptiope_small (e.g. \"something/something_else/adaptiope_small\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Transforms for resnet found there https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "  transform_img = list()\n",
        "  transform_img.append(T.Resize(256))\n",
        "  transform_img.append(T.CenterCrop(224))\n",
        "  transform_img.append(T.ToTensor())\n",
        "  transform_img.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "  transform_img = T.Compose(transform_img)\n",
        "\n",
        "  # load data\n",
        "  product_images_dataset = ImageFolder(root = f\"{root_dir}/product_images/\", transform = transform_img)\n",
        "  rw_images_dataset = ImageFolder(root = f\"{root_dir}/real_life/\", transform = transform_img)\n",
        "\n",
        "  product_train_indexes, product_test_indexes = train_test_split(list(range(len(product_images_dataset.targets))),\n",
        "                                                test_size = 0.2, stratify = product_images_dataset.targets, random_state = 42)\n",
        "  \n",
        "  rw_train_indexes, rw_test_indexes = train_test_split(list(range(len(rw_images_dataset.targets))),\n",
        "                                                test_size = 0.2, stratify = rw_images_dataset.targets, random_state = 42)\n",
        "  \n",
        "\n",
        "  product_train_data = Subset(product_images_dataset, product_train_indexes)\n",
        "  product_test_data = Subset(product_images_dataset, product_test_indexes)\n",
        "\n",
        "  rw_train_data = Subset(rw_images_dataset, rw_train_indexes)\n",
        "  rw_test_data = Subset(rw_images_dataset, rw_test_indexes)\n",
        "\n",
        "  product_train_loader = DataLoader(product_train_data, batch_size, shuffle = False)\n",
        "  product_test_loader = DataLoader(product_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  rw_train_loader = DataLoader(rw_train_data, batch_size, shuffle = False)\n",
        "  rw_test_loader = DataLoader(rw_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  return product_train_loader, product_test_loader, rw_train_loader, rw_test_loader\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wXlIpqRk8xZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network initialization\n",
        "The ResNet34 pretrained model is intialized. Since for the baseline we decided to perform a simple fine-tune, the original classifier layer has been overwritten and the gradients has been enabled."
      ],
      "metadata": {
        "id": "Q70XX70ALYR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_resnet34(num_classes: int, pretrained: Bool = True):\n",
        "  \"\"\"\n",
        "  resnet34 initialization\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  num_classes: int\n",
        "    number of categories the net should output\n",
        "\n",
        "  pretrained: bool\n",
        "    Specify if pretrained version of resnet should be retrieved\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  model = torchvision.models.resnet34(pretrained=pretrained)\n",
        "\n",
        "  in_features = model.fc.in_features\n",
        "\n",
        "  ##model.fc = nn.Sequential(nn.Linear(512, num_classes))#, nn.LogSoftmax(dim = 1))\n",
        "  model.fc = nn.Linear(512, num_classes)\n",
        "  for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Xq4k2RGV81M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-entropy loss for training data\n"
      ],
      "metadata": {
        "id": "takAN3EMLcj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ce_cost_function() -> torch.nn.CrossEntropyLoss:\n",
        "  \"\"\"\n",
        "  Simply returns cross entropy an object for computing the cross entropy loss\n",
        "  \"\"\"\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "F-mp8CmM83cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the optimizer\n",
        "We have written the abstract class `AnnealingOptimizer` in order to define an optimizer that updates the learning rate using the annealing strategy proposed in the [Symnet paper](https://arxiv.org/pdf/1904.04663.pdf).\n",
        "\n",
        "The strategy used is the following:\n",
        "$$\\eta = \\frac{\\eta_0}{(1+\\alpha p)^\\beta}$$\n",
        "where:\n",
        "\n",
        "- $\\eta_0$ is the base learning rate, which by default is 0.001. Note that it has been changed with respect to the one proposed in the paper, because we noticed that it was too high.\n",
        "\n",
        "- $p$ is the progress in training: $p = \\frac{epoch}{total-epochs}$. Notice that when the function `update_lr()` the value of $p$ gets updated.\n",
        "\n",
        "- $\\alpha = 10$ is a constant\n",
        "- $\\beta = 0.75$ is a constant\n",
        "\n",
        "Than the class `ResNetOptimizer` inherits from the `AnnealingOptimizer` and just defines the optimizer to be used. This optimizer will be used for ResNet34 which is the network of choice both for the baseline and the upper bound.\n",
        "\n",
        "<br>\n",
        "\n",
        "We decided to use the annealing optimizer also for the baseline and the upper bound, in order to compare it better with [Symnet](https://arxiv.org/pdf/1904.04663.pdf).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kN4WbrcENPOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class AnnealingOptimizer(torch.optim.Optimizer, ABC):\n",
        "  \"\"\"\n",
        "  Defines and abstract class in order to implement an sgd optimizer using an annealing strategy\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0) -> None:\n",
        "    if not 0.0 <= lr:\n",
        "      raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "    if not 0 <= epoch:\n",
        "      raise ValueError(f\"Invalid epoch value: {epoch}\")\n",
        "    \n",
        "    self.nr_epochs = nr_epochs\n",
        "    self.epoch = epoch\n",
        "    self._alpha = 10\n",
        "    self._beta = 0.75\n",
        "    self._base_lr = lr\n",
        "\n",
        "  def update_lr(self):\n",
        "    \"\"\"\n",
        "    Updates the learning rate using the annealing strategy.\n",
        "    In order to let the annealing strategy to work correctly, this method should be called at every epoch during the network training\n",
        "\n",
        "    The learning rate for the classifier is 10 times bigger as proposed in the [Symnet paper](https://arxiv.org/pdf/1904.04663.pdf)\n",
        "    \"\"\"\n",
        "    self.epoch += 1\n",
        "    new_lr = self._compute_lr()\n",
        "    for g in self.optimizer.param_groups:\n",
        "      if g[\"name\"] == \"fe\":\n",
        "        g[\"lr\"] = new_lr\n",
        "      else:\n",
        "        g[\"lr\"] = new_lr*10\n",
        "\n",
        "    \n",
        "  def _compute_lr(self):\n",
        "    \"\"\"\n",
        "    Computes the learning rate using the proposed annealing strategy\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      updated learning rate\n",
        "    \"\"\"\n",
        "    etap = 1 / ((1 + self._alpha * self.epoch / self.nr_epochs ) ** self._beta)\n",
        "    return self._base_lr * etap\n",
        "\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    self.optimizer.zero_grad()\n",
        "  \n",
        "  \n",
        "\n",
        "class ResNetOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for Resnet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0) -> None:\n",
        "    super(ResNetOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "    \n",
        "    # Note that names for parameters group are important in order to update each group differently\n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': self.__get_fe_params(model), \"name\": \"fe\"},\n",
        "                {'params': model.fc.parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=0.9)\n",
        "    \n",
        "\n",
        "  def __get_fe_params(self, model):\n",
        "    \"\"\"\n",
        "    Takes parameters of the Resnet's feature extractor\n",
        "    \"\"\"\n",
        "    fe_layers = list(model.children())[:-1]\n",
        "    all_parameters = [param for layer in fe_layers for param in layer.parameters()]\n",
        "    for param in all_parameters:\n",
        "      yield param"
      ],
      "metadata": {
        "id": "NEpkX1R0Mlwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "### Training procedure\n",
        "Briefly:\n",
        "- the net is set into train mode.\n",
        "\n",
        "- The training dataset is iteratively cycled through on groups of `batch_size` dimension. \n",
        "\n",
        "- For each sample in the current batch, inputs and targets are moved to the specified device, the predicted outputs and the losses computed.\n",
        "\n",
        "- After that, an optimization step is performed in order to update the weights.\n",
        "\n",
        "- Finally, accuracy and cumulative loss are computed"
      ],
      "metadata": {
        "id": "lBt70fF-LlWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_baseline(net, data_loader: torch.utils.data.DataLoader, optimizer,\n",
        "                           cost_function, device: str = 'cuda') -> tuple(float):\n",
        "\n",
        "  \"\"\"\n",
        "  Performs the training of the network for one epoch.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  net\n",
        "    network model.\n",
        "  \n",
        "  data_loader: torch.utils.data.DataLoader\n",
        "    Data loader intialized with the training set.\n",
        "  \n",
        "  optimizer: torch.optim.optimizer.Optimizer\n",
        "    Optimizer of choice\n",
        "\n",
        "  cost_function: torch.nn.modules._Loss\n",
        "    Loss function to be used\n",
        "\n",
        "  device: str\n",
        "    Device in which computations should be performed.\n",
        "    Admitted values:\n",
        "\n",
        "    - \"cpu\"\n",
        "\n",
        "    - \"cuda:n\" -> where n is the gpu number in case of multiple gpu configurations\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    A tuple of length 2 containing:\n",
        "    \n",
        "    - Cumulative loss for the whole training set\n",
        "\n",
        "    - Cumulative accuracy for the whole training set\n",
        "  \n",
        "\n",
        "  \"\"\"\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  net.train() \n",
        " \n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "\n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ],
      "metadata": {
        "id": "DMiSWtTz87wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test procedure\n",
        "- The network is set to evaluation mode. \n",
        "\n",
        "- After this, we disable all the gradients in order to avoid keeping track of the gradients (not needed for testing). The tesiting procedure from now on is pretty much analogous to what's been done during the training with the only difference that the weights of the network get not updated."
      ],
      "metadata": {
        "id": "IdP1pK-LLrlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "  \"\"\"\n",
        "  Test the network for one epoch\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  net\n",
        "    network model.\n",
        "  data_loader\n",
        "    Data loader intialized with the test set.\n",
        "  cost_function: torch.nn.modules._Loss\n",
        "    Loss function to be used\n",
        "  device: str\n",
        "    Device in which computations should be performed.\n",
        "    Admitted values:\n",
        "\n",
        "    - \"cpu\"\n",
        "\n",
        "    - \"cuda:n\" -> where n is the gpu number in case of multiple gpu configurations\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    A tuple of length 2 containing:\n",
        "    \n",
        "    - Cumulative loss for the whole training set\n",
        "\n",
        "    - Cumulative accuracy for the whole training set\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      \n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "9bHErRLpLuEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function (Product → Real World)\n",
        "This function is meant to be a 'wrapper' function where every aforecited function is called when needed.  \n",
        "First, the parameters values are defined as arguments of the function.  \n",
        "Then, sequentially :     \n",
        "- extract, process and load data;\n",
        "- network, optimizer and cost function initialization;\n",
        "- iterating a certain number of times equal to a fixed number of epochs. In here the following steps are performed :    \n",
        "  - computation of training loss and accuracy;\n",
        "  - computation of test loss and accuracy;\n",
        "  - informing the writer of the values obtained.\n",
        "\n",
        "- At the end of the training, the network is tested on both test sets of source and tagret domains.\n",
        "\n",
        "Here the network is trained on product images and then tested on real world ones"
      ],
      "metadata": {
        "id": "IiuMV7H_MUcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_PRD_to_RW(batch_size=128, \n",
        "         device='cuda', \n",
        "         learning_rate=0.0001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = 20, \n",
        "         img_root=\"gdrive/My Drive/Colab Notebooks/data/adaptiope_small\",\n",
        "         runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "         ):\n",
        "\n",
        "  writer = SummaryWriter(log_dir=runs_dir)\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  #optimizer = get_optimizer_SGD(net, learning_rate, wd = weight_decay, momentum = momentum)\n",
        "  optimizer = ResNetOptimizer(net, epochs)\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  print('Time to train!\\n==========================BASELINE========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    ##BASELINE\n",
        "\n",
        "\n",
        "    # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "    train_loss, train_accuracy = training_step_baseline(net, source_train_loader, optimizer, cost_function, device)\n",
        "    #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "    test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "    optimizer.update_lr()\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # add values to logger\n",
        "    \"\"\"writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\"\"\"\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, source_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "JDFkQoLq8-oD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_PRD_to_RW()"
      ],
      "metadata": {
        "id": "DGp93KtW9fDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/baseline/PRD2RW\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "E83UwVoYM0G7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Function (Real World → Product)\n",
        "Performs exactly the same process as in `main_PRD_to_RW()` just in the other direction: train on real world images and test in product images"
      ],
      "metadata": {
        "id": "UMt4Il3QMf3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_RW_to_PRD(batch_size=128, \n",
        "         device='cuda', \n",
        "         learning_rate=0.0001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = 20, \n",
        "         img_root=\"gdrive/My Drive/Colab Notebooks/data/adaptiope_small\",\n",
        "         runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "         ):\n",
        "\n",
        "  writer = SummaryWriter(log_dir=runs_dir)\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  target_train_loader, target_test_loader, source_train_loader, source_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  #optimizer = get_optimizer_SGD(net, learning_rate, wd = weight_decay, momentum = momentum)\n",
        "  optimizer = ResNetOptimizer(net, epochs)\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  print('Time to train!\\n==========================BASELINE========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    ##BASELINE\n",
        "\n",
        "\n",
        "    # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "    train_loss, train_accuracy = training_step_baseline(net, source_train_loader, optimizer, cost_function, device)\n",
        "    #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "    test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "    optimizer.update_lr()\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # add values to logger\n",
        "    \"\"\"writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\"\"\"\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, source_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "A5zN77uaMgsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_RW_to_PRD()"
      ],
      "metadata": {
        "id": "j7IYQVTJNrfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/baseline/RW2PRD\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "cock6OcuNslg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UPPER BOUND IMPLEMENTATION\n",
        "The *upper bound* consists in training supervisedly using the target domain's labels and testing on the target domain itself."
      ],
      "metadata": {
        "id": "St6XNk3SN74g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Product $\\to$ Real World\n",
        "H"
      ],
      "metadata": {
        "id": "c4aM8n3eOENz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_upper_bound(batch_size=BATCH_SIZE, \n",
        "                     device=cuda, \n",
        "                     learning_rate=0.0001, \n",
        "                     weight_decay=0.000001,\n",
        "                     momentum=0.9, \n",
        "                     epochs=15,\n",
        "                     entropy_loss_weight=0.1,\n",
        "                     nr_classes = num_classes, \n",
        "                     img_root=rootdir_matteo,\n",
        "                     runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "                     ):\n",
        "  \n",
        "  writer = SummaryWriter(log_dir=f\"{runsdir_matteo}/runs_upper_bound/PRD2RW\")\n",
        "\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = ResNetOptimizer()\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  \n",
        "  print('Time to train!\\n==========================UPPER BOUND========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    # Inspyred by : https://towardsdatascience.com/training-models-with-a-progress-a-bar-2b664de3e13e\n",
        "    with tqdm(source_train_loader, unit=\"batch\") as tepoch:\n",
        "      for data, target in tepoch:\n",
        "          tepoch.set_description(f\"Epoch {e}\")\n",
        "          # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "          train_loss, train_accuracy = training_step_baseline(net, target_train_loader, optimizer, cost_function, device)\n",
        "          #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "          test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "          tepoch.set_postfix(train_loss=train_loss, training_accuracy = train_accuracy, tst_loss = test_loss, tst_accuracy=test_accuracy)\n",
        "          sleep(0.1)\n",
        "\n",
        "    optimizer.update_lr()\n",
        "\n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, target_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "tCmjRfjGOInh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "knNUV3J-OOLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_upper_bound()"
      ],
      "metadata": {
        "id": "IQHeg9EAOOCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/runs_upper_bound/PRD2RW\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "YDHqcUovON1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real World $\\to$ Product"
      ],
      "metadata": {
        "id": "2n8SQNQcOUdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_UB_RW2PRD(batch_size=BATCH_SIZE, \n",
        "                     device=cuda, \n",
        "                     learning_rate=0.0001, \n",
        "                     weight_decay=0.000001,\n",
        "                     momentum=0.9, \n",
        "                     epochs=15,\n",
        "                     entropy_loss_weight=0.1,\n",
        "                     nr_classes = num_classes, \n",
        "                     img_root=rootdir_matteo,\n",
        "                     runs_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\"\n",
        "                     ):\n",
        "  \n",
        "  writer = SummaryWriter(log_dir=f\"{runsdir_matteo}/runs_upper_bound/RW2PRD\")\n",
        "\n",
        "  target_train_loader, target_test_loader, source_train_loader, source_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = ResNetOptimizer()\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  \n",
        "  print('Time to train!\\n==========================UPPER BOUND========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    # Inspyred by : https://towardsdatascience.com/training-models-with-a-progress-a-bar-2b664de3e13e\n",
        "    with tqdm(source_train_loader, unit=\"batch\") as tepoch:\n",
        "      for data, target in tepoch:\n",
        "          tepoch.set_description(f\"Epoch {e}\")\n",
        "          # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "          train_loss, train_accuracy = training_step_baseline(net, target_train_loader, optimizer, cost_function, device)\n",
        "          #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "          test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "          tepoch.set_postfix(train_loss=train_loss, training_accuracy = train_accuracy, tst_loss = test_loss, tst_accuracy=test_accuracy)\n",
        "          sleep(0.1)\n",
        "    \n",
        "    optimizer.update_lr()\n",
        "    \n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, target_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "Z748Ld_vOVnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "N7L6yQgiQG8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_UB_RW2PRD()"
      ],
      "metadata": {
        "id": "cDvEL0V4QIuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = f\"{runsdir_matteo}/runs_upper_bound/RW2PRD\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "fBfFRAUkQIkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Domain Adaptation Technique : SymNet\n",
        "The desgn of the proposed symmetric network is characterized by:\n",
        "\n",
        "- The Feature extractor G. We decided to use the feature extractor defined by ResNet34 (i.e. Resnet34 without the last fully connected layer) in order to allow the comparison with the results obtained with the baseline and the upper bound\n",
        "\n",
        "- Two parallel task classifiers $C_s$ and $C_t$ are both based on a single fully connected layer (as proposed in the paper) and they contain 20 neurons each, as the number of categories for the proposed problem.\n",
        "\n",
        "| ![symnet](https://drive.google.com/uc?id=1qyPClxz8zcJvhVGF84-IKv1Owljt0h7H) |\n",
        "|:--:|\n",
        "| *The image shows a sketch of the network architecture, including errors which will be explained later in this notebook.* |"
      ],
      "metadata": {
        "id": "qtwL0pU1QMvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SymNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Class representing the proposed symmetric network\n",
        "  \"\"\"\n",
        "  def __init__(self, n_classes: int = 20) -> None:\n",
        "    super(SymNet, self).__init__()\n",
        "    resnet = initialize_resnet34(20, True)\n",
        "    # Taking the feature extractor of resnet34\n",
        "    # Reference: https://stackoverflow.com/questions/55083642/extract-features-from-last-hidden-layer-pytorch-resnet18\n",
        "    self.feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "    self.source_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "    self.target_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "  \n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> tuple:\n",
        "    \"\"\"\n",
        "    Performs the forward pass\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Input tensor to the network\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "      The returned values are respectively the result of the source classifier, target classifier and the concatenation of the two.\n",
        "\n",
        "    \"\"\"\n",
        "    features = self.feature_extractor(x)\n",
        "    features = features.squeeze()\n",
        "    source_output = self.source_classifier(features)\n",
        "    # source_output = nn.Softmax(source_output)\n",
        "\n",
        "    target_output = self.target_classifier(features)\n",
        "    # target_output = nn.Softmax(target_output)\n",
        "\n",
        "    source_target_classifier = torch.cat((source_output, target_output), dim=1)\n",
        "    \n",
        "    return source_output , target_output, source_target_classifier\n",
        "  \n",
        "  def parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Paramters of the netowork\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Network parameter\n",
        "    \"\"\"\n",
        "    fe = list(self.feature_extractor.parameters())\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = fe + sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n",
        "    \n",
        "  def classifier_parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Parameters of the classification layer\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Classification layer parameter\n",
        "    \"\"\"\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n",
        "\n",
        "  def feature_extractor_parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Parameters of the feature extractor\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Feature extractor parameter\n",
        "    \"\"\"\n",
        "    return self.feature_extractor.parameters()\n"
      ],
      "metadata": {
        "id": "jw9TzdJA8--0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimizer for symnet\n",
        "Symnet uses the AnnealingOptimizer strategy in order to adjust the learning rate during epochs as proposed in the paper.\n",
        "It just defines the optimizer with the right parameters.\n",
        "Additionaly, again following the paper, the learning rate for the combined classifiers (i.e. $C_{st}$) is set 10 times bigger than the feature extractor learning rate."
      ],
      "metadata": {
        "id": "ipknhK-jJ9YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SymNetOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for SymNet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0):\n",
        "    super(SymNetOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "\n",
        "    # Note that names for parameters group are important in order to update each group differently\n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': model.feature_extractor_parameters(), \"name\": \"fe\"},\n",
        "                {'params': model.classifier_parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=0.9)\n",
        "  \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "s0ZPPxfrXsuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notation used\n",
        "In order to define the losses, we define the notation used for formulas:\n",
        "\n",
        "- The classifiers are denoted as: $C^s$ for the source classifier, $C^t$ for the target classifier and $C^{st}$ for the combined classifier (source + target)\n",
        "\n",
        "- $K$ is the ouput dimension of each classifier, which corresponds to the number of categories\n",
        "($K=K_s=K_t= \\# \\:\\: of \\:\\: categories$)\n",
        "\n",
        "- $v^s(x) \\in R^K$, $v^t(x) \\in R^K$ and $[v^s(x),v^t(x)] \\in R^{2k}$ the output vectors of $C^s$, $C^t$ and $C^{st}$ respectevely **before the softmax operation**\n",
        "\n",
        "- $p^s(x) \\in [0,1]^K$, $p^t(x) \\in [0,1]^K$ and $p^{st} \\in [0,1]^{2K}$ the output vectors of $C^s$, $C^t$ and $C^{st}$ respectevely **after the softmax operation**. To denote the $k^{th}$ element of the vector the following notation is used: $p^s_k$ (resp. $p^t_k$ and $p^{st}_k$), $k \\in \\{1,...,K\\}$<br>\n",
        "_Note: $p^{st}$ is computed considering 2K classes, so it is not equal to the concatenation of $p^s$ and $p^t$_\n",
        "\n",
        "\n",
        "\n",
        "### Definining the losses\n",
        "[Symnet Paper](https://arxiv.org/pdf/1904.04663.pdf) defines two losses:\n",
        "- One for updating the weights of the three classifier ($C^s$, $C^t$ and $C^{st}$). We refer to this loss as \"Classifier loss\"\n",
        "\n",
        "- The other for updating the weights of the feture extractor. We refere to this loss as \"feature extractor loss\"\n",
        "\n",
        "In the following sections, these two losses will be explained in greater detail\n",
        "\n",
        "*Note: remember that the weights for $C^{st}$  are shared with the other two classifiers*\n",
        "\n",
        "#### Classifier loss\n",
        "The objective for updating the classifiers weigths is the following:\n",
        "\n",
        "$$\\min_{C^s, C^t, C^{st}} \\mathcal{E}^s_{task}(G,C^s) + \\mathcal{E}^t_{task}(G,C^t) + \\mathcal{E}^{st}_{domain}(G,C^{st})$$\n",
        "\n",
        "It is possible to notice that the the whole objective is composed of three errors which are defined as follows:\n",
        "\n",
        "- Error for the task classifier is simple cross entropy but considering only the output corresponding to the true category ($y^s_i$) $$\\mathcal{E}^s_{task}(G,C^s) = - \\frac{1}{n_s}\\sum_{i=1}^{n_s}{log(p^s_{y^s_i}(x^s_i))}$$\n",
        "\n",
        "- The same thing it is done for the target classifier, using again the source training samples (since labels for the target are not given): $$\\mathcal{E}^s_{task}(G,C^s) = - \\frac{1}{n_s}\\sum_{i=1}^{n_s}{log(p^t_{y^s_i}(x^s_i))}$$\n",
        "The use of this loss is essential to provide the correspondance between $C^s$ and C^t in order to allow the achievement of category-level domain confusion which will be optained later using one of the errors for the update of the classifier\n",
        "\n",
        "- By using only these two errors, $C^s$ and $C^t$ learn the exact same thing, so the third error, which acts on the combined classifier $C^{st}$, is needed to distinguish between the two: $$\\mathcal{E}^{st}_{domain}(G,C^{st}) = - \\frac{1}{n_t}\\sum_{j=1}^{n_t}{log(\\sum_{k=1}^{K}{p^{st}_{K+k}(x^t_j)})} \\\\ -\\frac{1}{n_s}\\sum_{i=1}^{n_s}{log(\\sum_{k=1}^{K}{p^{st}_{k}(x^s_i)})}$$ It's is important to notice that this is completely computed in an unsupervised manner, so it is possible to take advantage also of target training samples. It is also possible to see $\\sum_{k=1}^{K}{p^{st}_{K+k}(x^t_j)}$ and $\\sum_{k=1}^{K}{p^{st}_{k}(x^s_i)}$ as the probability of classify an input sample x as target or source respectively.\n",
        "\n",
        "#### Feature extractor loss\n",
        "\n",
        "As in other strategies for adversarial training in domain adapatation, the aim is to find a feature extractor G that is invariant to the domain (So the aim is to find a feature extractor that can generalize better). To do that, the [paper](https://arxiv.org/pdf/1904.04663.pdf) proposes a \"two-level domain confusion\" method based on a domain-level confusion loss and a category-level confusion loss.\n",
        "The objective for updating the feature extractor loss, is the following:\n",
        "\n",
        "$$\\min_{G} \\mathcal{F}^{st}_{category}(G, C^{st}) + \\lambda (\\mathcal{F}^{st}_{domain}(G, C^{st}) + \\mathcal{M}^{st}(G, C^{st}))$$\n",
        "\n",
        "Where $\\lambda \\in [0,1]$ is a tradeoff parameters to suppress noisy signals of $\\mathcal{F}^{st}_{domain}(G, C^{st})$ and $\\mathcal{M}^{st}(G, C^{st}))$ at early stages of training. This is because at the beginning, convolutional features aren't extracting meaningful information (since the network is not trained yet), so we need better convolutional features before starting to confuse them.\n",
        "\n",
        "As for the classifier objective, it is possible to distiniguish three distinct terms:\n",
        "\n",
        "- "
      ],
      "metadata": {
        "id": "4tgsuh-A5ghN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def source_loss(output, label):\n",
        "  \"\"\"\n",
        "  Returns\n",
        "  -------\n",
        "  Cross entropy loss\n",
        "  \"\"\"\n",
        "  loss_fun = nn.CrossEntropyLoss()\n",
        "  loss = loss_fun(output, label)\n",
        "  return loss\n",
        "\n",
        "def target_loss(output, label):\n",
        "  return source_loss(output, label)\n",
        "\n",
        "def source_target_loss(output, st = True):\n",
        "  \"\"\"\n",
        "  st = True if train sample belongs to source, False otherwise\n",
        "  \"\"\"\n",
        "  n_classes = int(output.size(1)/2)\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output)\n",
        "  if st:\n",
        "    loss = -(prob_out[:,:n_classes].sum(1).log().mean())\n",
        "  else:\n",
        "    loss = -(prob_out[:,n_classes:].sum(1).log().mean())\n",
        "  return loss\n",
        "\n",
        "def feature_category_loss(output_st, label):\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  loss_fun_1 = nn.CrossEntropyLoss()\n",
        "  loss_fun_2 = nn.CrossEntropyLoss()\n",
        "\n",
        "  loss_1 = loss_fun_1(output_st[:, :n_classes], label)/2\n",
        "  loss_2 = loss_fun_2(output_st[:,n_classes:], label)/2\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "def feature_domain_loss(output_st):\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output_st)\n",
        "\n",
        "  loss_1 = -(prob_out[:,:n_classes]).sum(1).log().mean()/2\n",
        "  loss_2 = -(prob_out[:,n_classes:]).sum(1).log().mean()/2\n",
        "\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "\n",
        "\n",
        "def entropyMinimizationPrinciple(output_st):\n",
        "    nr_classes = int(output_st.size(1)/2)\n",
        "    soft = nn.Softmax(dim=1)\n",
        "    prob_out = soft(output_st)\n",
        "\n",
        "    p_st_source = prob_out[:, :nr_classes]\n",
        "    p_st_target = prob_out[:, nr_classes:]\n",
        "    qst = p_st_source + p_st_target\n",
        "\n",
        "    emp = -qst.log().mul(qst).sum(1).mean()\n",
        "\n",
        "    return emp"
      ],
      "metadata": {
        "id": "cSa0gFgR9HEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_uda(net, src_data_loader, target_data_loader, optimizer, lam, e,device = 'cuda'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_classifier_loss = 0.\n",
        "  cumulative_feature_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  target_iter = iter(target_data_loader)\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs_source, labels) in enumerate(src_data_loader):\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    except:\n",
        "      target_iter = iter(target_data_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs_source = inputs_source.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    length_source_input = inputs_source.shape[0]\n",
        "\n",
        "    ## concatenation along batch dimension.\n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "\n",
        "    # forward pass\n",
        "    c_s, c_t, c_st = net(inputs)\n",
        "\n",
        "    c_s_source = c_s[:length_source_input,:]\n",
        "    c_s_target = c_s[length_source_input:,:]\n",
        "\n",
        "    c_t_source = c_t[:length_source_input,:]\n",
        "    c_t_target = c_t[length_source_input:,:]\n",
        "\n",
        "    c_st_source = c_st[:length_source_input,:]\n",
        "    c_st_target = c_st[length_source_input:,:]\n",
        "\n",
        "\n",
        "    # Equation 5 of the paper\n",
        "    error_source_task = source_loss(c_s_source, labels)\n",
        "\n",
        "    # Equation 6 of the paper\n",
        "    error_target_task = target_loss(c_t_source, labels)\n",
        "\n",
        "    # Equation 7 of the paper\n",
        "    domain_loss_source = source_target_loss(c_st_source)\n",
        "    domain_loss_target = source_target_loss(c_st_target, st = False)\n",
        "    error_domain = domain_loss_source + domain_loss_target\n",
        "\n",
        "    classifier_total_loss = error_source_task + error_target_task + error_domain\n",
        "\n",
        "    classifier_total_loss.backward(retain_graph = True)\n",
        "\n",
        "    for param in net.feature_extractor.parameters():\n",
        "      param.grad.data.zero_()\n",
        "    \n",
        "    class_params = []\n",
        "    for param in net.source_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "    for param in net.target_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "\n",
        "    # Equation 8 of the paper\n",
        "    error_feature_category = feature_category_loss(c_st_source, labels)\n",
        "\n",
        "    # Equation 9 of the paper\n",
        "    error_feature_domain = feature_domain_loss(c_st_target)\n",
        "\n",
        "    min_entropy = entropyMinimizationPrinciple(c_st_target)\n",
        "\n",
        "    # Equations 11 of the paper\n",
        "    feature_total_loss = error_feature_category + lam * (error_feature_domain + min_entropy)\n",
        "\n",
        "    feature_total_loss.backward()\n",
        "\n",
        "    idx = 0\n",
        "    for param in net.source_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "    for param in net.target_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "\n",
        "    \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    ## optimizer classifier losses composed loss\n",
        "    ## order is important here!\n",
        "    \n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    source_samples+=inputs_source.shape[0]\n",
        "    target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_classifier_loss += classifier_total_loss.item()\n",
        "    cumulative_feature_loss += feature_total_loss.item()\n",
        "    _, predicted = c_s_source.max(dim = 1) ## to get the maximum probability\n",
        "    cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "  return cumulative_classifier_loss/source_samples, cumulative_feature_loss/target_samples, cumulative_accuracy/source_samples*100\n"
      ],
      "metadata": {
        "id": "dGdsgnes9HcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(net, data_target_test_loader, device='cuda:0'):\n",
        "\n",
        "    '''\n",
        "    Params\n",
        "    ------\n",
        "\n",
        "    net : model \n",
        "    data_loader : DataLoader obj of the domain to test on\n",
        "    cost_function : cost function used to address accuracies (not necessary) -> TargetClassifierLoss\n",
        "    device : GPU or CPU device\n",
        "\n",
        "    '''\n",
        "\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(data_target_test_loader):\n",
        "\n",
        "            # load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = labels.to(device)\n",
        "        \n",
        "            # forward pass\n",
        "            _, c_t, _ = net(inputs)\n",
        "\n",
        "            # apply the loss\n",
        "            loss = target_loss(c_t, targets)\n",
        "\n",
        "            # print statistics\n",
        "            samples+=inputs.shape[0]\n",
        "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "            _, predicted = c_t.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "hQiFy0TZ9j3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['backpack', 'bookcase', 'car jack', 'comb', 'crown', 'file cabinet', 'flat iron', 'game controller', 'glasses', 'helicopter', 'ice skates', 'letter tray', 'monitor', 'mug', 'network switch', 'over-ear headphones', 'pen', 'purse', 'stand mixer', 'stroller']\n",
        "\n",
        "cuda = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 128\n",
        "num_classes = len(classes)\n",
        "rootdir_matteo = '/content/gdrive/MyDrive/Colab Notebooks/Deep Learning labs/DA Project/adaptiope_small'\n",
        "rootdir_alessandro = 'gdrive/My Drive/Colab Notebooks/data/adaptiope_small'\n",
        "rootdir_alessandro_uni = 'gdrive/My Drive/project/data/adaptiope_small'"
      ],
      "metadata": {
        "id": "ixVFg1jb9mLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Product $\\to$ Real World"
      ],
      "metadata": {
        "id": "IxD91oKuQg0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "def main_uda(batch_size=128,\n",
        "         device=cuda, \n",
        "         lr = 0.01,\n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=40,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = num_classes, \n",
        "         img_root=rootdir_alessandro\n",
        "         ):\n",
        "    \n",
        "  # writer = SummaryWriter(log_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\")\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = SymNet().to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = SymNetOptimizer(model = net, nr_epochs = epochs) #get_optimizer_ADAM_uda(model=net, e=0, nr_epochs = epochs,lr=lr, wd=weight_decay)\n",
        "  # optimizer_2 = get_optimizer_ADAM_uda(model=net, lr=lr, wd=weight_decay, e=0, nr_epochs=epochs, classifier=False)\n",
        "  print('Got optimizers')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    lam = 2 / (1 + math.exp(-1 * 10 * e / epochs)) - 1\n",
        "    #def training_step_uda(net, src_data_loader, target_data_loader, optimizer_1, optimizer_2, lam, device = 'cuda')\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n",
        "                                                        target_data_loader=target_train_loader, \n",
        "                                                        optimizer=optimizer, lam=lam, e=e, device=device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    test_loss, test_accuracy = test_step(net, target_test_loader, device)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    optimizer.update_lr()"
      ],
      "metadata": {
        "id": "h2-rMG7B9u6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JrQkjYQQQncm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_uda()"
      ],
      "metadata": {
        "id": "sLOVe5Q99xmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kill 490\n",
        "runs = f\"{runsdir_matteo}/DA/PRD2RW\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "metadata": {
        "id": "Jy8EPD-l92UX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Real World $\\to$ Product"
      ],
      "metadata": {
        "id": "5FS6g9wxQwuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_uda_RW2PRD(batch_size=128,\n",
        "         device=cuda, \n",
        "         lr = 0.01,\n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=30,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = num_classes, \n",
        "         img_root=rootdir_matteo\n",
        "         ):\n",
        "    \n",
        "  writer = SummaryWriter(log_dir=f\"{runsdir_matteo}/DA/RW2PRD\")\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  target_train_loader, target_test_loader, source_train_loader, source_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = SymNet().to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = SymNetOptimizer(model = net, nr_epochs = epochs) #get_optimizer_ADAM_uda(model=net, e=0, nr_epochs = epochs,lr=lr, wd=weight_decay)\n",
        "  # optimizer_2 = get_optimizer_ADAM_uda(model=net, lr=lr, wd=weight_decay, e=0, nr_epochs=epochs, classifier=False)\n",
        "  print('Got optimizers')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    lam = 2 / (1 + math.exp(-1 * 10 * e / epochs)) - 1\n",
        "    #def training_step_uda(net, src_data_loader, target_data_loader, optimizer_1, optimizer_2, lam, device = 'cuda')\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n",
        "                                                        target_data_loader=target_train_loader, \n",
        "                                                        optimizer=optimizer, lam=lam, e=e, device=device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    test_loss, test_accuracy = test_step(net, target_test_loader, device)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_ce_loss', train_ce_loss, e + 1)\n",
        "    writer.add_scalar('Loss/train_en_loss', train_en_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "\n",
        "\n",
        "    optimizer.update_lr()\n",
        "  \n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "2WTz1wd0QxHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_5SY9Je0Q1I2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_uda_RW2PRD()"
      ],
      "metadata": {
        "id": "jmqwwlefQ3oh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}