{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/Symnet-Unsupervised-domain-adaptation/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_Conventions and notes:_\n",
        "\n",
        "- _Some small sentences of the paper are copied exactly in this document. This is done in parts when we felt that there were no reason to rewrite, better explain or summarise the concept because it was already clear and concise for us._\n",
        "\n",
        "- _For better coherence we decided to stick with the notatation used in the paper in writing formulas._\n",
        "\n",
        "- _Wherever code is not commented, it is because we thought that it was something trivial._\n",
        "\n",
        "- _Somewhere you can find no markdown preceeding a code cell, this is because the included code lines were either discussed in class or a similar chunk of code was already explained earlier or a docstring was more suitable for the context._\n",
        "\n",
        "- _If there is any problem in the visualization of images, here is the link to the github repository, where images should be displayed correctly: [Symnet github](https://github.com/Zinni98/DL-Project/blob/main/project.ipynb)._\n",
        "\n",
        "# Unsupervised Domain Adaptation\n",
        "## 1. Introduction\n",
        "The **goal** of this project is to **build a deep learning framework for** Unsupervised Domain Adaptation (**UDA**).\n",
        "**Domain Adaptation** is a subdiscipline of machine learning which deals with scenarios where a **model is trained supervisedly on** data coming from a known distribution (*source* domain) **but** during the **test** phase, data are sampled from **another unknown distribution** (*target* domain). This of course can impact the performance singnificantly.\n",
        "The *underlying concept* of DA is *closely related to transfer learning* which refers to a class of ML problems that *deals with different tasks or domains*. \n",
        "Furthermore, it is possible to use the target data without labels (that's why \"unsupervised\") in order to improve the performance of the model.\n",
        "\n",
        "## 2. Objective\n",
        "The **aim** of this project is to **use** an **Unsupervised Domain Adpatation technique of choice** in order **to improve** performances with respect to a **baseline**. The latter is obtained by training on the source domain and testing on the target domain without deploying any specific technique to take the domain shift into account.\n",
        "\n",
        "### 2.1 UDA Method\n",
        "The **method chosen** for Unsepervised Domain Adaptation is [**Symnet**](https://arxiv.org/pdf/1904.04663.pdf) (explained later in more details) which belongs to the **domain adversarial family** of methods for domain adaptation. The main idea is to **use a two-level domain confusion scheme** in order to let the intermediate network learn features that are **invariant to the corresponding category in both domains**.\n",
        "\n",
        "### 2.2 Problem setting\n",
        "In this problem we are required to perform an **object recognition task**: given an image, the model should be able to produce the label, hopefully correct, associated with that image.\n",
        "\n",
        "### 2.2.1 Datasets\n",
        "The chosen **dataset** is [**Adaptiope**](https://openaccess.thecvf.com/content/WACV2021/papers/Ringwald_Adaptiope_A_Modern_Benchmark_for_Unsupervised_Domain_Adaptation_WACV_2021_paper.pdf) which has images coming from **3 domains and 123 classes**.\n",
        "We are going to **use** a subset of this consisting on **2 domains with 20 classes**. The two domains are:\n",
        "\n",
        "- **Real world**\n",
        "\n",
        "- **Product images**\n",
        "\n",
        "As simplifying assumption the **20 classes are the same across the two domains**.\n",
        "\n",
        "From now on we will treat the data coming from the two domains as two separate datasets: the **source dataset $X^S$** and the **target dataset $X^T$**.\n",
        "Both $X^S$ and $X^T$ use a 80%/20% split for the training and test set respectively.\n",
        "\n",
        "*Note : Here we refer to Source and Target instead of Real World and Product because the domain adaptation procedure is applied both ways, i.e. first the real world domain dataset is considered as training set and the target domain dataset as test set. Consequently, the reverse has been addressed as well.*\n",
        "\n",
        "[comment]: <> (\n",
        "_Note: Here we refer to Source and Target instead of Real World and Product, because during training we are going evaluate our model first by considering real world domain as source and product as target, and then the opposite._\n",
        ")\n",
        "\n",
        "### 2.2.2 Methodology\n",
        "To evaluate performances of the proposed domain adaptation technique, we need to define a *baseline score* which we want *to improve* on. The baseline score is obtained by firstly training on the source training set, and then evaluating on the target training set.\n",
        "\n",
        "| ![Baseline](https://drive.google.com/uc?id=15EdVkmyHUD_sXPuPDE2P0XB5jk-dn9t0) |\n",
        "|:--:|\n",
        "| *The image shows a sketch with the highlighted parts used to train and test the baseline* |\n",
        "\n",
        "It can be *useful* also *to define an upper bound* to the performance, obtained by *training on the target training set* and *testing on the target test set*.\n",
        "\n",
        "| ![Baseline](https://drive.google.com/uc?id=1Y-jM7yED_ssufwL-hAGNTlbr_KntTCSZ) |\n",
        "|:--:|\n",
        "| *The image shows a sketch with the highlighted parts used to train and test the upper bound* |\n",
        "\n",
        "Finally, for **Unsupervised Domain Adaptation the training is performed supervisedly on the source training set**, **unsupervisedly on the target training set** and then **tested on the target test set**.\n",
        "\n",
        "| ![Baseline](https://drive.google.com/uc?id=17Wmg7Yd7RltaOBAkA5DEksTp4HUTJvrT) |\n",
        "|:--:|\n",
        "| *The image shows a sketch with the highlighted parts used to train and test the UDA technique* |\n",
        "\n",
        "We **expect** that the **proposed** Domain Adaptation **technique will perform better than the baseline** but worse than the upper bound and of course, the more close is to the upper bound, the better.\n",
        "\n",
        "Performance are measured in terms of **validation accuracy**: \n",
        "$$\\text{Accuracy} = \\dfrac{TP+TN}{TP+TN+FP+FN}$$\n",
        "\n",
        "In this project we are required to test the proposed Unsupervised Domain Adaptation technique first by considering the \"Product\" domain as the source domain and the \"Real World\" domain as the target domain and then to the other way around: \"Real World\" as source and \"Product\" as target.\n",
        "So we need to compute baseline, upper bound and UDA performances first in one direction (i.e. Product $\\rightarrow$ Real World) and then in the other direction (i.e. Real World $\\rightarrow$ Product)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hYWXadIfJkAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive  # to mount personal drive\n",
        "\n",
        "\n",
        "from tqdm import tqdm   # for progress bar \n",
        "from time import sleep\n",
        "\n",
        "import torch  # importing pytorch\n",
        "import torch.optim as optim  # importing optimizer module\n",
        "from torch.utils.data import Subset  # useful in defining data of interest in a dataset\n",
        "import torch.nn as nn  # Neural Network tools\n",
        "from torch.utils.tensorboard import SummaryWriter # to get plots of trends\n",
        "from torch.utils.data import DataLoader\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T  # to apply transformations to dataset images\n",
        "from torchvision.datasets import ImageFolder  # to load and applying transformations on data\n",
        "#import torchvision.transforms.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split  # to split a dataset into training and test set\n",
        "\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from typing import Tuple, List"
      ],
      "metadata": {
        "id": "XDSUxCL6Jwj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "WVUMK-gw8w8V",
        "outputId": "ff98862f-2b19-4808-9eb6-afcf737db184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['backpack', 'bookcase', 'car jack', 'comb', 'crown', 'file cabinet', 'flat iron', 'game controller', 'glasses', 'helicopter', 'ice skates', 'letter tray', 'monitor', 'mug', 'network switch', 'over-ear headphones', 'pen', 'purse', 'stand mixer', 'stroller']\n",
        "\n",
        "cuda = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 128\n",
        "num_classes = len(classes)\n",
        "rootdir = 'gdrive/My Drive/Colab Notebooks/data/adaptiope_small'\n",
        "# rootdir_alessandro_uni = 'gdrive/My Drive/project/data/adaptiope_small'\n",
        "runs_dir = 'gdrive/My Drive/Colab Notebooks/runs/'"
      ],
      "metadata": {
        "id": "84aVoKluJ8CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Extraction\n",
        "In `get_data(batch_size, root_dir)` the following steps are performed :\n",
        "- images *transforms are defined*. In particular, the adopted transformation sequence has been found there: [ResNet Transforms](https://pytorch.org/hub/pytorch_vision_resnet/);\n",
        "- *images* from the local drive are *loaded and the transforms applied*;\n",
        "- data *splitting*;\n",
        "- *collecting individual fetched data samples into batches*.\n",
        "The returned objects are the real world and product domain data loaders."
      ],
      "metadata": {
        "id": "z4uiw_OkK6tN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(batch_size: int, root_dir: str, random_state = 42, test_split = 0.2) -> Tuple[torch.utils.data.DataLoader]:\n",
        "  \"\"\"\n",
        "\n",
        "  Params:\n",
        "  ------\n",
        "  batch_size: int\n",
        "    batch size for the dataloader\n",
        "  root_dir: str\n",
        "    Directory of adaptiope_small (e.g. \"something/something_else/adaptiope_small\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Transforms for resnet found there https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "  transform_img = list()\n",
        "  transform_img.append(T.Resize(256))\n",
        "  transform_img.append(T.CenterCrop(224))\n",
        "  transform_img.append(T.ToTensor())\n",
        "  transform_img.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "  transform_img = T.Compose(transform_img)\n",
        "\n",
        "  # load data\n",
        "  product_images_dataset = ImageFolder(root = f\"{root_dir}/product_images/\", transform = transform_img)\n",
        "  rw_images_dataset = ImageFolder(root = f\"{root_dir}/real_life/\", transform = transform_img)\n",
        "\n",
        "  product_train_indexes, product_test_indexes = train_test_split(list(range(len(product_images_dataset.targets))),\n",
        "                                                test_size = test_split, stratify = product_images_dataset.targets, random_state = random_state)\n",
        "  \n",
        "  rw_train_indexes, rw_test_indexes = train_test_split(list(range(len(rw_images_dataset.targets))),\n",
        "                                                test_size = test_split, stratify = rw_images_dataset.targets, random_state = random_state)\n",
        "  \n",
        "\n",
        "  product_train_data = Subset(product_images_dataset, product_train_indexes)\n",
        "  product_test_data = Subset(product_images_dataset, product_test_indexes)\n",
        "\n",
        "  rw_train_data = Subset(rw_images_dataset, rw_train_indexes)\n",
        "  rw_test_data = Subset(rw_images_dataset, rw_test_indexes)\n",
        "\n",
        "  product_train_loader = DataLoader(product_train_data, batch_size, shuffle = False)\n",
        "  product_test_loader = DataLoader(product_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  rw_train_loader = DataLoader(rw_train_data, batch_size, shuffle = False)\n",
        "  rw_test_loader = DataLoader(rw_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  return product_train_loader, product_test_loader, rw_train_loader, rw_test_loader\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wXlIpqRk8xZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Baseline\n",
        "### 4.1 Network initialization\n",
        "The **ResNet34 pretrained model is intialized**. Since for the **baseline** we decided to perform a **simple fine-tune**, the original classifier layer has been overwritten and the gradients has been enabled."
      ],
      "metadata": {
        "id": "Q70XX70ALYR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_resnet34(num_classes: int):\n",
        "  \"\"\"\n",
        "  resnet34 initialization\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  num_classes: int\n",
        "    number of categories the net should output\n",
        "\n",
        "  pretrained: bool\n",
        "    Specify if pretrained version of resnet should be retrieved\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  model = torchvision.models.resnet34(weights='ResNet34_Weights.DEFAULT')\n",
        "\n",
        "  in_features = model.fc.in_features\n",
        "\n",
        "  ##model.fc = nn.Sequential(nn.Linear(512, num_classes))#, nn.LogSoftmax(dim = 1))\n",
        "  model.fc = nn.Linear(512, num_classes)\n",
        "  for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Xq4k2RGV81M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Cross-entropy loss for training data\n"
      ],
      "metadata": {
        "id": "takAN3EMLcj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ce_cost_function() -> torch.nn.CrossEntropyLoss:\n",
        "  \"\"\"\n",
        "  Simply returns cross entropy an object for computing the cross entropy loss\n",
        "  \"\"\"\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "F-mp8CmM83cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Defining the optimizer\n",
        "We have written the abstract class `AnnealingOptimizer` in order to define an optimizer that updates the learning rate using the annealing strategy proposed in the [Symnet paper](https://arxiv.org/pdf/1904.04663.pdf).\n",
        "\n",
        "The strategy used is the following:\n",
        "$$\\eta = \\frac{\\eta_0}{(1+\\alpha p)^\\beta}$$\n",
        "where:\n",
        "\n",
        "- $\\eta_0$ is the **base learning rate**, which by default is $0.001$. Note that it has been *changed with respect to the one proposed in the paper*, because we noticed that it was too high ;\n",
        "\n",
        "- $p$ is the progress in training: $\\text{p} = \\frac{\\text{epoch}}{\\text{total epochs}}$. Notice that when the function `update_lr()` is called, the value of $p$ gets updated ;\n",
        "\n",
        "- $\\alpha = 10$ is a constant ;\n",
        "- $\\beta = 0.75$ is a constant ;\n",
        "\n",
        "Than the **class `ResNetOptimizer`** inherits from the `AnnealingOptimizer` and just **defines the optimizer** to be used. This optimizer will be used for **ResNet34**, which is the network of choice, either for the baseline, the upper bound and the deployed UDA technique.\n",
        "\n",
        "<br>\n",
        "\n",
        "We decided to use the **annealing optimizer** also **for** the **baseline and the upper bound**, in order **to compare it better with** [Symnet](https://arxiv.org/pdf/1904.04663.pdf).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kN4WbrcENPOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class AnnealingOptimizer(torch.optim.Optimizer, ABC):\n",
        "  \"\"\"\n",
        "  Defines and abstract class in order to implement an sgd optimizer using an annealing strategy\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0) -> None:\n",
        "    if not 0.0 <= lr:\n",
        "      raise ValueError(f\"Invalid learning rate: {lr}\")\n",
        "    if not 0 <= epoch:\n",
        "      raise ValueError(f\"Invalid epoch value: {epoch}\")\n",
        "    \n",
        "    self.nr_epochs = nr_epochs\n",
        "    self.epoch = epoch\n",
        "    self._alpha = 10\n",
        "    self._beta = 0.75\n",
        "    self._base_lr = lr\n",
        "\n",
        "  def update_lr(self):\n",
        "    \"\"\"\n",
        "    Updates the learning rate using the annealing strategy.\n",
        "    In order to let the annealing strategy to work correctly, this method should be called at every epoch during the network training\n",
        "\n",
        "    The learning rate for the classifier is 10 times bigger as proposed in the [Symnet paper](https://arxiv.org/pdf/1904.04663.pdf)\n",
        "    \"\"\"\n",
        "    self.epoch += 1\n",
        "    new_lr = self._compute_lr()\n",
        "    for g in self.optimizer.param_groups:\n",
        "      if g[\"name\"] == \"fe\":\n",
        "        g[\"lr\"] = new_lr\n",
        "      else:\n",
        "        g[\"lr\"] = new_lr*10\n",
        "\n",
        "    \n",
        "  def _compute_lr(self):\n",
        "    \"\"\"\n",
        "    Computes the learning rate using the proposed annealing strategy\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "      updated learning rate\n",
        "    \"\"\"\n",
        "    etap = 1 / ((1 + self._alpha * self.epoch / self.nr_epochs ) ** self._beta)\n",
        "    return self._base_lr * etap\n",
        "\n",
        "  def step(self):\n",
        "    self.optimizer.step()\n",
        "  \n",
        "  def zero_grad(self):\n",
        "    self.optimizer.zero_grad()\n",
        "  \n",
        "  \n",
        "\n",
        "class ResNetOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for Resnet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0, momentum: float = 0.9) -> None:\n",
        "    super(ResNetOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "    \n",
        "    # Note that names for parameters group are important in order to update each group differently\n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': self.__get_fe_params(model), \"name\": \"fe\"},\n",
        "                {'params': model.fc.parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=momentum)\n",
        "    \n",
        "\n",
        "  def __get_fe_params(self, model):\n",
        "    \"\"\"\n",
        "    Takes parameters of the Resnet's feature extractor\n",
        "    \"\"\"\n",
        "    fe_layers = list(model.children())[:-1]\n",
        "    all_parameters = [param for layer in fe_layers for param in layer.parameters()]\n",
        "    for param in all_parameters:\n",
        "      yield param"
      ],
      "metadata": {
        "id": "NEpkX1R0Mlwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Training procedure\n",
        "Briefly :\n",
        "- the **net** is set into **train mode**.\n",
        "\n",
        "- The **training dataset is iteratively cycled** through on groups of `batch_size` dimension. \n",
        "\n",
        "- **For each sample** in the current batch, **inputs and targets** are **moved** to the specified **device**, the predicted outputs and the losses computed.\n",
        "\n",
        "- After that, an **optimization step** is performed **to update the weights**.\n",
        "\n",
        "- Finally, **accuracy and cumulative loss are computed**."
      ],
      "metadata": {
        "id": "lBt70fF-LlWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step(net, data_loader: torch.utils.data.DataLoader, optimizer,\n",
        "                           cost_function, device: str = cuda) -> Tuple[float]:\n",
        "\n",
        "  \"\"\"\n",
        "  Performs the training of the network for one epoch.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  net\n",
        "    network model.\n",
        "  \n",
        "  data_loader: torch.utils.data.DataLoader\n",
        "    Data loader intialized with the training set.\n",
        "  \n",
        "  optimizer: torch.optim.optimizer.Optimizer\n",
        "    Optimizer of choice\n",
        "\n",
        "  cost_function: torch.nn.modules._Loss\n",
        "    Loss function to be used\n",
        "\n",
        "  device: str\n",
        "    Device in which computations should be performed.\n",
        "    Admitted values:\n",
        "\n",
        "    - \"cpu\"\n",
        "\n",
        "    - \"cuda:n\" -> where n is the gpu number in case of multiple gpu configurations\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    A tuple of length 2 containing:\n",
        "    \n",
        "    - Cumulative loss for the whole training set\n",
        "\n",
        "    - Cumulative accuracy for the whole training set\n",
        "  \n",
        "\n",
        "  \"\"\"\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  net.train() \n",
        " \n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "\n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n"
      ],
      "metadata": {
        "id": "DMiSWtTz87wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Test procedure\n",
        "- The **network** is set to **evaluation mode**. \n",
        "\n",
        "- After this, we **disable all the gradients** to avoid keeping track of the gradients (not needed for testing). The tesiting procedure from now on is pretty much analogous to what's been done during the training with the only difference that the **weights of the network don't get updated**."
      ],
      "metadata": {
        "id": "IdP1pK-LLrlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(net, data_loader, cost_function, device=cuda):\n",
        "  \"\"\"\n",
        "  Test the network for one epoch\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  net\n",
        "    network model.\n",
        "  data_loader\n",
        "    Data loader intialized with the test set.\n",
        "  cost_function: torch.nn.modules._Loss\n",
        "    Loss function to be used\n",
        "  device: str\n",
        "    Device in which computations should be performed.\n",
        "    Admitted values:\n",
        "\n",
        "    - \"cpu\"\n",
        "\n",
        "    - \"cuda:n\" -> where n is the gpu number in case of multiple gpu configurations\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  tuple\n",
        "    A tuple of length 2 containing:\n",
        "    \n",
        "    - Cumulative loss for the whole training set\n",
        "\n",
        "    - Cumulative accuracy for the whole training set\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      \n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "9bHErRLpLuEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Main Function\n",
        "This function is meant to be a '**wrapper**' **function** where every aforecited function is called when needed.  \n",
        "First, the **parameters values are defined as arguments** of the function.  \n",
        "Then, sequentially :     \n",
        "- extract, **process** and load **data**;\n",
        "- network, optimizer and cost function are **initialized**;\n",
        "- **iterating** a certain number of times equal to a **fixed number of epochs**. In here the following steps are performed :    \n",
        "  - **computation of training loss and accuracy**;\n",
        "  - **computation of test loss and accuracy**;\n",
        "  - informing the writer of the values obtained.\n",
        "\n",
        "- At the **end of the training**, the **network** is **tested on** both test sets of **source and tagret** domains.\n",
        "\n",
        "Here the network is trained on product images and then tested on real world ones."
      ],
      "metadata": {
        "id": "IiuMV7H_MUcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(train_loader,\n",
        "         test_loader,\n",
        "         batch_size = BATCH_SIZE, \n",
        "         device = cuda,\n",
        "         epochs = 15,\n",
        "         nr_classes = num_classes, \n",
        "         img_root = rootdir,\n",
        "         runs_dir=runs_dir,\n",
        "         ):\n",
        "  \"\"\"\n",
        "  Parameters\n",
        "  ----------\n",
        "  batch_size\n",
        "    Dimension of the batch for the single optimization step\n",
        "\n",
        "  device\n",
        "    The device on which the computation takes place.\n",
        "    Admitted values:\n",
        "\n",
        "    - \"cpu\"\n",
        "\n",
        "    - \"cuda:n\" -> where n is the gpu number in case of multiple gpu configurations\n",
        "\n",
        "  epochs\n",
        "    Number of training epochs\n",
        "\n",
        "  nr_classes\n",
        "    Number of classes for the classification task\n",
        "  \n",
        "  img_root\n",
        "    Root where the dataset is stored\n",
        "  \n",
        "  runs_dir\n",
        "    Directory for saving the results of runs\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = ResNetOptimizer(net, epochs)\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "\n",
        "  # writer = SummaryWriter(log_dir=f\"{runs_dir}/runs_upper_bound/RW2PRD\")\n",
        "\n",
        "  for e in range(epochs):\n",
        "    print(f\"Epoch {e}:\")\n",
        "    train_loss, train_accuracy = training_step(net, train_loader, optimizer, cost_function, device)\n",
        "    print(f\"Training loss: {train_loss} \\n Training accuracy: {train_accuracy}\")\n",
        "    # Needed to apply the annealing strategy\n",
        "    optimizer.update_lr()\n",
        "\n",
        "    # add values to logger\n",
        "   # writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "   # writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  _, test_accuracy = test_step(net, test_loader, cost_function, device)\n",
        "\n",
        "  # close the logger\n",
        "  # writer.close()\n",
        "\n",
        "  return test_accuracy\n"
      ],
      "metadata": {
        "id": "zfEjGzde22ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Domain Adaptation Technique : SymNet\n",
        "The **design** of the **proposed symmetric network** is characterized by:\n",
        "\n",
        "- The **Feature extractor G**. We decided to use the *feature extractor defined by ResNet34* (i.e. Resnet34 without the last fully connected layer) to make the comparison with the baseline and upper bound results much fair as possible ;\n",
        "\n",
        "[comment]: <> (in order to allow the comparison with the results obtained with the baseline and the upper bound)\n",
        "\n",
        "- **Two parallel task classifiers** $C_s$ and $C_t$ are both based on a single fully connected layer (as proposed in the paper) and they contain **$20$ neurons each**, as the number of categories for the proposed problem (When we will explain the losses used for Symnet, we are going to use $K$ to denote the number of classes in order to be more generic). \n",
        "Composing the two classifiers, we get the $C_{st}$ classifer that presents a total of $40$ units, e.g. the union of the two FC layers.\n",
        "\n",
        "| ![symnet](https://drive.google.com/uc?id=1qyPClxz8zcJvhVGF84-IKv1Owljt0h7H) |\n",
        "|:--:|\n",
        "| *The image shows a sketch of the network architecture, including the error functions which will be explained later on.* |\n",
        "\n",
        "<br>\n",
        "\n",
        "As it is possible to notice, the **architecture** results to be **pretty simple**. \n",
        "Indeed, the **core reasoning** has been developped on **losses definitions level**."
      ],
      "metadata": {
        "id": "qtwL0pU1QMvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SymNet(nn.Module):\n",
        "  \"\"\"\n",
        "  Class representing the proposed symmetric network\n",
        "  \"\"\"\n",
        "  def __init__(self, n_classes: int = 20) -> None:\n",
        "    super(SymNet, self).__init__()\n",
        "    resnet = initialize_resnet34(20, True)\n",
        "    # Taking the feature extractor of resnet34\n",
        "    # Reference: https://stackoverflow.com/questions/55083642/extract-features-from-last-hidden-layer-pytorch-resnet18\n",
        "    self.feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "    self.source_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "    self.target_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "  \n",
        "\n",
        "  def forward(self, x: torch.Tensor) -> tuple:\n",
        "    \"\"\"\n",
        "    Performs the forward pass\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x : torch.Tensor\n",
        "      Input tensor to the network\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "      The returned values are respectively the result of the source classifier, target classifier and the concatenation of the two.\n",
        "\n",
        "    \"\"\"\n",
        "    features = self.feature_extractor(x)\n",
        "    features = features.squeeze()\n",
        "    source_output = self.source_classifier(features)\n",
        "    # source_output = nn.Softmax(source_output)\n",
        "\n",
        "    target_output = self.target_classifier(features)\n",
        "    # target_output = nn.Softmax(target_output)\n",
        "\n",
        "    source_target_classifier = torch.cat((source_output, target_output), dim=1)\n",
        "    \n",
        "    return source_output , target_output, source_target_classifier\n",
        "  \n",
        "  def parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Paramters of the netowork\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Network parameter\n",
        "    \"\"\"\n",
        "    fe = list(self.feature_extractor.parameters())\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = fe + sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n",
        "    \n",
        "  def classifier_parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Parameters of the classification layer\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Classification layer parameter\n",
        "    \"\"\"\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n",
        "\n",
        "  def feature_extractor_parameters(self) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Parameters of the feature extractor\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    torch.Tensor\n",
        "      Feature extractor parameter\n",
        "    \"\"\"\n",
        "    return self.feature_extractor.parameters()\n"
      ],
      "metadata": {
        "id": "jw9TzdJA8--0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Optimizer for symnet\n",
        "Symnet uses the **AnnealingOptimizer strategy** in order **to adjust the learning rate during epochs** as proposed in the paper.\n",
        "\n",
        "[comment]: <> (It just defines the optimizer with the right parameters.)\n",
        "\n",
        "Additionaly, again following the paper, the **learning rate for** the combined classifier (e.g. **$C_{st}$**) is set **$10$ times bigger than the feature extractor one**."
      ],
      "metadata": {
        "id": "ipknhK-jJ9YB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SymNetOptimizer(AnnealingOptimizer):\n",
        "  \"\"\"\n",
        "  Implements an annealing optimizer for SymNet\n",
        "  \"\"\"\n",
        "  def __init__(self, model, nr_epochs, lr: float = 0.001, epoch: int = 0):\n",
        "    super(SymNetOptimizer ,self).__init__(model, nr_epochs, lr, epoch)\n",
        "\n",
        "    # Note that names for parameters group are important in order to update each group differently\n",
        "    self.optimizer = optim.SGD([\n",
        "                {'params': model.feature_extractor_parameters(), \"name\": \"fe\"},\n",
        "                {'params': model.classifier_parameters(), \"lr\": self._compute_lr()*10, \"name\": \"classifier\"}\n",
        "            ], lr=lr, momentum=0.9)"
      ],
      "metadata": {
        "id": "s0ZPPxfrXsuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Notation used\n",
        "In order to define the losses, we explicit here the notation used in the following formulas:\n",
        "\n",
        "- The classifiers are denoted as: **$C^s$** for the **source classifier**, **$C^t$** for the **target classifier** and **$C^{st}$** for the **combined classifier** (source + target) ;\n",
        "\n",
        "- **$K$** is the **ouput dimension** of each classifier (source and target), which corresponds to the number of categories in both domains. Since in UDA the number of classes in both domains is the same we get :\n",
        "$K=K_s=K_t= \\# \\:\\: of \\:\\: categories$;\n",
        "\n",
        "- $v^s(x) \\in R^K$, $v^t(x) \\in R^K$ and $[v^s(x),v^t(x)] \\in R^{2k}$ are the output vectors of $C^s$, $C^t$ and $C^{st}$, respectevely, **before the softmax operation** ;\n",
        "\n",
        "- $p^s(x) \\in [0,1]^K$, $p^t(x) \\in [0,1]^K$ and $p^{st} \\in [0,1]^{2K}$ are the output vectors of $C^s$, $C^t$ and $C^{st}$, respectevely, **after the softmax operation**. To denote the $k^{th}$ element of the vector the following notation is used: $p^s_k$ (resp. $p^t_k$ and $p^{st}_k$), $k \\in \\{1,...,K\\}$. <br>\n",
        "_Note: $p^{st}$ is computed considering 2K classes, so it is not equal to the concatenation of $p^s$ and $p^t$_\n",
        "\n",
        "<br>\n",
        "\n",
        "### Definining the losses\n",
        "[Symnet Paper](https://arxiv.org/pdf/1904.04663.pdf) defines **two losses**:\n",
        "- **One for updating the weights of the three classifier** ($C^s$, $C^t$ and $C^{st}$). We refer to this loss as \"Classifier loss\"\n",
        "\n",
        "- The **other for updating the weights of the feture extractor**. We refere to this loss as \"feature extractor loss\"\n",
        "\n",
        "In the following sections, these two losses will be explained in greater detail\n",
        "\n",
        "*Note: remember that the weights for $C^{st}$  are shared with the other two classifiers*\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Classifier loss\n",
        "The **objective for updating the classifiers weigths** is the following:\n",
        "\n",
        "$$\\min_{C^s, C^t, C^{st}} \\mathcal{E}^s_{task}(G,C^s) + \\mathcal{E}^t_{task}(G,C^t) + \\mathcal{E}^{st}_{domain}(G,C^{st})$$\n",
        "\n",
        "It is possible to notice that the the whole **objective** is **composed of three errors** which are defined as follows:\n",
        "\n",
        "- **Error for the task classifier** is simple **cross entropy** but considering only the output corresponding to the true category ($y^s_i$) $$\\mathcal{E}^s_{task}(G,C^s) = - \\frac{1}{n_s}\\sum_{i=1}^{n_s}{log(p^s_{y^s_i}(x^s_i))}$$\n",
        "\n",
        "\n",
        "- The **same thing** is done **for the target classifier**. In this case, since **no direct supervision to learn task classfier $C_t$** is available, the **labelled source samples are leveraged** as follows: $$\\mathcal{E}^t_{task}(G,C^t) = - \\frac{1}{n_s}\\sum_{i=1}^{n_s}{log(p^t_{y^s_i}(x^s_i))}$$\n",
        "The use of **this loss** is **essential to provide the correspondance between $C^s$ and $C^t$** in order to allow the **achievement of category-level domain confusion** which will be obtained later using one of the errors for the update of the classifier.\n",
        "\n",
        "- By using only these two errors, $C^s$ and $C^t$ learn the exact same thing, so the **third error**, which acts on the combined classifier $C^{st}$, is **needed to distinguish between the two**: $$\\mathcal{E}^{st}_{domain}(G,C^{st}) = - \\frac{1}{n_t}\\sum_{j=1}^{n_t}{\\log\\bigg(\\sum_{k=1}^{K}{p^{st}_{K+k}(x^t_j)}\\bigg)} \\\\ -\\frac{1}{n_s}\\sum_{i=1}^{n_s}{\\log\\bigg(\\sum_{k=1}^{K}{p^{st}_{k}(x^s_i)}\\bigg)}$$ \n",
        "It's is important to notice that this is **completely computed in an unsupervised manner**, so it is possible to take advantage also of target training samples. It is also possible to see $\\sum_{k=1}^{K}{p^{st}_{K+k}(x^t_j)}$ and $\\sum_{k=1}^{K}{p^{st}_{k}(x^s_i)}$ as the probability of classify an input sample **x** as target or source respectively.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### Feature extractor loss\n",
        "\n",
        "As in other strategies for adversarial training in domain adapatation, the **aim** is to find a **feature extractor G that is invariant to the domain**; in other words we are seeking to find a feature extractor that can generalize better. **To do that**, the [paper](https://arxiv.org/pdf/1904.04663.pdf) proposes a \"**two-level domain confusion**\" method based on a domain-level confusion loss and a category-level confusion loss.\n",
        "The objective for updating the feature extractor loss, is the following:\n",
        "\n",
        "$$\\min_{G} \\mathcal{F}^{st}_{category}(G, C^{st}) + \\lambda (\\mathcal{F}^{st}_{domain}(G, C^{st}) + \\mathcal{M}^{st}(G, C^{st}))$$\n",
        "\n",
        "Where $\\lambda \\in [0,1]$ is a **trade-off parameters to suppress noisy signals** of $\\mathcal{F}^{st}_{domain}(G, C^{st})$ and $\\mathcal{M}^{st}(G, C^{st}))$ **at early stages of training**. *This is because* at the beginning, convolutional features aren't extracting meaningful information (since the network is not trained yet), so *we need better convolutional features before starting to confuse them*.\n",
        "\n",
        "As for the classifier objective, it is possible to distiniguish three distinct terms:\n",
        "\n",
        "- **Category-level confusion loss** using **labeled source samples**: $$\\mathcal{F}^{st}_{category}(G, C^{st}) = -\\frac{1}{2n_s}\\sum_{i=1}^{n_s}{\\log(p^{st}_{y^s_i + K}(x^s_i))} \\\\ -\\frac{1}{2n_s}\\sum_{i=1}^{n_s}{\\log(p^{st}_{y^s_i}(x^s_i))}$$\n",
        "\n",
        "- **Domain-level confusion loss** using **unlabeled target samples**:\n",
        "$$\\mathcal{F}^{st}_{domain}(G,C^{st}) = - \\frac{1}{2n_t}\\sum_{j=1}^{n_t}{\\log\\bigg(\\sum_{k=1}^{K}{p^{st}_{K+k}(x^t_j)}\\bigg)} \\\\ -\\frac{1}{2n_t}\\sum_{j=1}^{n_t}{\\log\\bigg(\\sum_{k=1}^{K}{p^{st}_{k}(x^s_j)}\\bigg)}$$\n",
        "\n",
        "- **Entropy minimization principle**:\n",
        "$$\\mathcal{M}^{st}(G, C^{st}) = - \\frac{1}{n_t}\\sum_{j=1}^{n_t}\\sum_{k=1}^{K}q^{st}_k(x^t_j)log(q^{st}_k(x^t_j))$$\n",
        "The above entropy minimization objective enhances discrimination among task categories"
      ],
      "metadata": {
        "id": "4tgsuh-A5ghN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def source_loss(output_source, label):\n",
        "  \"\"\"\n",
        "   Cross entropy loss of source classifier C_s for source samples (equation 5 of the paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the source classifier\n",
        "  \n",
        "  label: torch.Tensor\n",
        "    Labels corresponding to the samples whose output is computed\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "  \"\"\"\n",
        "  loss_fun = nn.CrossEntropyLoss()\n",
        "  loss = loss_fun(output_source, label)\n",
        "  return loss\n",
        "\n",
        "def target_loss(output_target, label):\n",
        "  \"\"\"\n",
        "  Cross entropy loss of target classifier C_t for source samples (equation 6 of the paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the target classifier\n",
        "  \n",
        "  label: torch.Tensor\n",
        "    Labels corresponding to the samples whose output is computed\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "  \"\"\"\n",
        "  return source_loss(output_target, label)\n",
        "\n",
        "def source_target_loss(output, st = True):\n",
        "  \"\"\"\n",
        "  Two-way cross-entropy loss for the joint classifier C_st (equation 7 of the paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the combined source-target classifier\n",
        "  st: bool\n",
        "    True if train batch belongs to source, False if belongs to target\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "\n",
        "  \"\"\"\n",
        "  n_classes = int(output.size(1)/2)\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output)\n",
        "  if st:\n",
        "    loss = -(prob_out[:,:n_classes].sum(1).log().mean())\n",
        "  else:\n",
        "    loss = -(prob_out[:,n_classes:].sum(1).log().mean())\n",
        "  return loss\n",
        "\n",
        "def feature_category_loss(output_st, label):\n",
        "  \"\"\"\n",
        "  Category level confusion loss (equation 8 of the Symnet paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output_st: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the combined source-target classifier\n",
        "  \n",
        "  label: torch.Tensor\n",
        "    Labels corresponding to the samples whose output is computed\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "\n",
        "  \"\"\"\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  loss_fun_1 = nn.CrossEntropyLoss()\n",
        "  loss_fun_2 = nn.CrossEntropyLoss()\n",
        "\n",
        "  loss_1 = loss_fun_1(output_st[:, :n_classes], label)/2\n",
        "  loss_2 = loss_fun_2(output_st[:,n_classes:], label)/2\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "def feature_domain_loss(output_st):\n",
        "  \"\"\"\n",
        "  Domain level confusion loss (equation 9 of the Symnet paper)\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  output: torch.Tensor\n",
        "    Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "    be the output of the combined source-target classifier\n",
        "  \n",
        "  Returns\n",
        "  -------\n",
        "  torch.Tensor\n",
        "    The result of the computed loss for the entire batch\n",
        "\n",
        "  \"\"\"\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output_st)\n",
        "\n",
        "  loss_1 = -(prob_out[:,:n_classes]).sum(1).log().mean()/2\n",
        "  loss_2 = -(prob_out[:,n_classes:]).sum(1).log().mean()/2\n",
        "\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "\n",
        "\n",
        "def entropyMinimizationPrinciple(output_st):\n",
        "    \"\"\"\n",
        "    Entropy minimization principle (equation 10 of the Symnet paper)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.Tensor\n",
        "      Output batch of the network. Notice that in order to let the algorithm work correctly, this should\n",
        "      be the output of the combined source-target classifier\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "      The corresponding entropy minimization loss for the entire batch\n",
        "    \"\"\"\n",
        "    nr_classes = int(output_st.size(1)/2)\n",
        "    soft = nn.Softmax(dim=1)\n",
        "    prob_out = soft(output_st)\n",
        "\n",
        "    p_st_source = prob_out[:, :nr_classes]\n",
        "    p_st_target = prob_out[:, nr_classes:]\n",
        "    qst = p_st_source + p_st_target\n",
        "\n",
        "    emp = -qst.log().mul(qst).sum(1).mean()\n",
        "\n",
        "    return emp"
      ],
      "metadata": {
        "id": "cSa0gFgR9HEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4 Training step\n",
        "_Note: The high level procedure, is very similar to the one proposed for training the baseline and the upper bound (i.e. forward-step/compute-loss/backward-step/update-gradients/zeroing-gradients) but there are some details that are worth mentioning, so in general we will focus on these details._\n",
        "\n",
        "The following is the process that we used to train Symnet:\n",
        "\n",
        "- For each single training step we decided to give an input batch containing half source samples and half target samples, this is done in order to keep each batch balanced.\n",
        "\n",
        "- Now it comes the tricky part of computing the gradients and updating the weights:\n",
        "\n",
        "  1. First the classifier loss is computed (Equations 5, 6 and 7 of the paper)\n",
        "  2. Then the gradients of the feature extractor are set to 0 because we don't want to update the weights of the feature extractor with gradients computed using the classifier loss.\n",
        "  3. After that, we need to save the computed gradients for both classifiers, otherwise when performing the backward step of the feature extractor loss, the gradients of the classifiers would be overwritten.\n",
        "  4. Now we can safely compute the feature extractor loss and perform the backward pass.\n",
        "  5. After that we just need to manually overwrite the computed gradients for the classifier (Note that ad these point the gradients are the ones computed on the feature extractor loss) with the previously saved gradients (**step 3**)\n",
        "  6. Finally an optimizer step plus zeroing gradients, can be safely computed assuring a correct functioning of the network.\n",
        "\n",
        "- Cumulative accuracy and loss are finally computed\n",
        "\n",
        "### Other methods tried (but not correct):\n",
        "We tried other strategies in order to update gradients, but non of those worked:\n",
        "\n",
        "#### 1<sup>st</sup> trial\n",
        "Using only one optimizer but without saving weigths:\n",
        "\n",
        "1. Compute classifier loss\n",
        "2. Perform the backward pass\n",
        "3. Update weights: `optimizer.step()`\n",
        "4. zeroing the gradients: `optimizer.zero_grad()`\n",
        "5. Perform the same thing for the feature extractor loss\n",
        "6. ....\n",
        "\n",
        "The problem with this method is that we update the feature extractor parameters with the first loss (other than classifier's ones) and the classifier parameters with the second loss (other than feature extractor's ones), which doesn't make sense, otherwise we wouldn't needed two separate losses for updating the two separate groups of parameters in the first place.\n",
        "\n",
        "#### 2<sup>nd</sup> trial\n",
        "Use two optimizers to update one group of parameters each at a time (_Note: we will call optimizer1 the optimizer for classifier weigths and optimizer2 the one for feature extractor weigths_):\n",
        "\n",
        "1. Compute classifier loss\n",
        "2. Perform the backward pass\n",
        "3. Update classifier weights: `optimizer1.step()`\n",
        "4. Zeroing optimizer1 gradients: `optimizer1.zero_grad()`\n",
        "5. Compute feature extractor loss\n",
        "6. Compute the backward pass\n",
        "7. Update feature extractor weights: `optimizer2.step()`\n",
        "8. Zeroing optimizer2 gradients: `optimizer2.zero_grad()`\n",
        "\n",
        "The problem with this approach is that an error occurs when computing the second backward pass (bullet point 6). This appens because pytorch keeps track of the version number of the tensor, which is incremented when performing an in-place operation on the tensor value (__Attention: not the value of the gradient, but the actual tensor__). So when we are updating the classifier weights (bullet point 3), the tensors corresponding to the weights of the classifier, get updated, this implies that they will have a different version number with respect to the ones of the feature extractor.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Wp_DebGtVAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_uda(net, src_data_loader, target_data_loader, optimizer, lam, e, device=cuda):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_classifier_loss = 0.\n",
        "  cumulative_feature_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  target_iter = iter(target_data_loader)\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs_source, labels) in enumerate(src_data_loader):\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    except:\n",
        "      target_iter = iter(target_data_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs_source = inputs_source.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    length_source_input = inputs_source.shape[0]\n",
        "\n",
        "    ## concatenation along batch dimension.\n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "\n",
        "    # forward pass\n",
        "    c_s, c_t, c_st = net(inputs)\n",
        "\n",
        "    c_s_source = c_s[:length_source_input,:]\n",
        "    c_s_target = c_s[length_source_input:,:]\n",
        "\n",
        "    c_t_source = c_t[:length_source_input,:]\n",
        "    c_t_target = c_t[length_source_input:,:]\n",
        "\n",
        "    c_st_source = c_st[:length_source_input,:]\n",
        "    c_st_target = c_st[length_source_input:,:]\n",
        "\n",
        "\n",
        "    # Equation 5 of the paper\n",
        "    error_source_task = source_loss(c_s_source, labels)\n",
        "\n",
        "    # Equation 6 of the paper\n",
        "    error_target_task = target_loss(c_t_source, labels)\n",
        "\n",
        "    # Equation 7 of the paper\n",
        "    domain_loss_source = source_target_loss(c_st_source)\n",
        "    domain_loss_target = source_target_loss(c_st_target, st = False)\n",
        "    error_domain = domain_loss_source + domain_loss_target\n",
        "\n",
        "    classifier_total_loss = error_source_task + error_target_task + error_domain\n",
        "\n",
        "    # Retain graph needed because otherwise the parts of the computation graph\n",
        "    # needed to compute classifier_total_loss will be freed up, but we\n",
        "    # need those parts in order to compute the next loss\n",
        "    classifier_total_loss.backward(retain_graph = True)\n",
        "\n",
        "    for param in net.feature_extractor.parameters():\n",
        "      param.grad.data.zero_()\n",
        "    \n",
        "    class_params = []\n",
        "    for param in net.source_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "    for param in net.target_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "\n",
        "    # Equation 8 of the paper\n",
        "    error_feature_category = feature_category_loss(c_st_source, labels)\n",
        "\n",
        "    # Equation 9 of the paper\n",
        "    error_feature_domain = feature_domain_loss(c_st_target)\n",
        "\n",
        "    min_entropy = entropyMinimizationPrinciple(c_st_target)\n",
        "\n",
        "    # Equations 11 of the paper\n",
        "    feature_total_loss = error_feature_category + lam * (error_feature_domain + min_entropy)\n",
        "\n",
        "    feature_total_loss.backward()\n",
        "\n",
        "    idx = 0\n",
        "    for param in net.source_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "    for param in net.target_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "\n",
        "    \n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    source_samples+=inputs_source.shape[0]\n",
        "    target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_classifier_loss += classifier_total_loss.item()\n",
        "    cumulative_feature_loss += feature_total_loss.item()\n",
        "    _, predicted = c_s_source.max(dim = 1) ## to get the maximum probability\n",
        "    cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "  return cumulative_classifier_loss/source_samples, cumulative_feature_loss/target_samples, cumulative_accuracy/source_samples*100\n"
      ],
      "metadata": {
        "id": "dGdsgnes9HcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.6 Test step\n",
        "The test step is very similar to the ones proposed for baseline and upper bound.\n",
        "The only detail is that for us is worth mentioning, is that the predictions considered are the ones of the target classifier $C^t$."
      ],
      "metadata": {
        "id": "wQ1he8AA4y4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step_uda(net, data_target_test_loader, device=cuda):\n",
        "\n",
        "    '''\n",
        "    Params\n",
        "    ------\n",
        "\n",
        "    net : model \n",
        "    data_loader : DataLoader obj of the domain to test on\n",
        "    cost_function : cost function used to address accuracies (not necessary) -> TargetClassifierLoss\n",
        "    device : GPU or CPU device\n",
        "\n",
        "    '''\n",
        "\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(data_target_test_loader):\n",
        "\n",
        "            # load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = labels.to(device)\n",
        "        \n",
        "            # forward pass\n",
        "            _, c_t, _ = net(inputs)\n",
        "\n",
        "            # apply the loss\n",
        "            loss = target_loss(c_t, targets)\n",
        "\n",
        "            # print statistics\n",
        "            samples+=inputs.shape[0]\n",
        "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "            _, predicted = c_t.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "hQiFy0TZ9j3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.7 Main"
      ],
      "metadata": {
        "id": "IxD91oKuQg0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "def main_uda(source_train_loader,\n",
        "             target_train_loader,\n",
        "             target_test_loader,\n",
        "             device=cuda,\n",
        "             epochs=15,\n",
        "             nr_classes = num_classes, \n",
        "             img_root=rootdir,\n",
        "            ):\n",
        "    \n",
        "  # writer = SummaryWriter(log_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\")\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  \n",
        "  print('DataLoaders Done')\n",
        "  net = SymNet().to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer = SymNetOptimizer(model = net, nr_epochs = epochs)\n",
        "  print('Got optimizers')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    lam = 2 / (1 + math.exp(-1 * 10 * e / epochs)) - 1\n",
        "\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n",
        "                                                        target_data_loader=target_train_loader, \n",
        "                                                        optimizer=optimizer, lam=lam, e=e, device=device)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    print(f'Epoch: {e+1:d}')\n",
        "    print(f'\\t Train: CE loss {train_ce_loss:.5f}, Entropy loss {train_en_loss:.5f}, Accuracy {train_accuracy:.2f}')\n",
        "    optimizer.update_lr()\n",
        "\n",
        "  test_loss, test_accuracy = test_step_uda(net, target_test_loader, device)\n",
        "  return test_accuracy"
      ],
      "metadata": {
        "id": "h2-rMG7B9u6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluation of the proposed method\n",
        "### 6.1 Product $\\to$ Real World\n",
        "Here we are comparing the proposed method with the baseline and the upper bound, by using the product dataset as source and the real world dataset as target.\n",
        "First we need to get the dataloaders:"
      ],
      "metadata": {
        "id": "AfVLlx65JWSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset:\n",
        "product_train_loader, product_test_loader, rw_train_loader, rw_test_loader = get_data(BATCH_SIZE, rootdir)"
      ],
      "metadata": {
        "id": "eV0fqOEskL2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1.1 Baseline"
      ],
      "metadata": {
        "id": "xj4IrhddkKc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_base = main(product_train_loader, rw_test_loader, runs_dir=runs_dir)\n",
        "print(f\"Baseline accuracy Product -> Real World: {acc_base}\")"
      ],
      "metadata": {
        "id": "N_3ZFdPVJSyX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1b4734-03cd-4ff2-f9e5-69e99f5167a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "DataLoaders Done\n",
            "Time to train!\n",
            "==========================BASELINE========================\n",
            "Epoch 0:\n",
            "Training loss: 0.01795370575040579 \n",
            " Training accuracy: 40.25\n",
            "Epoch 1:\n",
            "Training loss: 0.00427818444557488 \n",
            " Training accuracy: 91.9375\n",
            "Epoch 2:\n",
            "Training loss: 0.0017345193773508072 \n",
            " Training accuracy: 96.3125\n",
            "Epoch 3:\n",
            "Training loss: 0.0012162890005856753 \n",
            " Training accuracy: 97.75\n",
            "Epoch 4:\n",
            "Training loss: 0.000965243827085942 \n",
            " Training accuracy: 98.5625\n",
            "Epoch 5:\n",
            "Training loss: 0.000824839398264885 \n",
            " Training accuracy: 98.9375\n",
            "Epoch 6:\n",
            "Training loss: 0.0007277771341614426 \n",
            " Training accuracy: 99.1875\n",
            "Epoch 7:\n",
            "Training loss: 0.0006549248937517405 \n",
            " Training accuracy: 99.3125\n",
            "Epoch 8:\n",
            "Training loss: 0.0005981729598715901 \n",
            " Training accuracy: 99.375\n",
            "Epoch 9:\n",
            "Training loss: 0.00055244832765311 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 10:\n",
            "Training loss: 0.0005146433413028717 \n",
            " Training accuracy: 99.5\n",
            "Epoch 11:\n",
            "Training loss: 0.00048276068177074196 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 12:\n",
            "Training loss: 0.0004554457706399262 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 13:\n",
            "Training loss: 0.000431778043275699 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 14:\n",
            "Training loss: 0.00041105804382823407 \n",
            " Training accuracy: 99.75\n",
            "Baseline accuracy Product -> Real World: 78.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1.2 Upper bound"
      ],
      "metadata": {
        "id": "MjCyXauQIwqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_upperbound = main(rw_train_loader, rw_test_loader, img_root=rootdir_alessandro, runs_dir=runs_dir_alessandro, mode=\"u\")\n",
        "print(f\"Upper Bound accuracy Product -> Real World: {acc_upperbound}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGrdoiWMIvuL",
        "outputId": "60003f4f-4b85-41c9-bd92-c5dc4e17f4b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "DataLoaders Done\n",
            "Time to train!\n",
            "==========================UPPER BOUND========================\n",
            "Epoch 0:\n",
            "Training loss: 0.02011194162070751 \n",
            " Training accuracy: 29.562500000000004\n",
            "Epoch 1:\n",
            "Training loss: 0.007576549611985684 \n",
            " Training accuracy: 82.4375\n",
            "Epoch 2:\n",
            "Training loss: 0.003562918156385422 \n",
            " Training accuracy: 91.8125\n",
            "Epoch 3:\n",
            "Training loss: 0.00237793386913836 \n",
            " Training accuracy: 95.8125\n",
            "Epoch 4:\n",
            "Training loss: 0.0019154228921979665 \n",
            " Training accuracy: 96.75\n",
            "Epoch 5:\n",
            "Training loss: 0.0016147012542933226 \n",
            " Training accuracy: 97.625\n",
            "Epoch 6:\n",
            "Training loss: 0.00141404977068305 \n",
            " Training accuracy: 98.125\n",
            "Epoch 7:\n",
            "Training loss: 0.0012683934019878506 \n",
            " Training accuracy: 98.375\n",
            "Epoch 8:\n",
            "Training loss: 0.001151873115450144 \n",
            " Training accuracy: 98.625\n",
            "Epoch 9:\n",
            "Training loss: 0.0010581590654328466 \n",
            " Training accuracy: 98.875\n",
            "Epoch 10:\n",
            "Training loss: 0.000981118776835501 \n",
            " Training accuracy: 99.125\n",
            "Epoch 11:\n",
            "Training loss: 0.0009162570117041468 \n",
            " Training accuracy: 99.25\n",
            "Epoch 12:\n",
            "Training loss: 0.0008607942564412952 \n",
            " Training accuracy: 99.3125\n",
            "Epoch 13:\n",
            "Training loss: 0.0008128180494531989 \n",
            " Training accuracy: 99.5\n",
            "Epoch 14:\n",
            "Training loss: 0.0007709031458944082 \n",
            " Training accuracy: 99.5\n",
            "Upper Bound accuracy Product -> Real World: 91.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.1.3 Domain Adaptation"
      ],
      "metadata": {
        "id": "EtxCiNF0I9a_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_da = main_uda(product_train_loader, rw_train_loader, rw_test_loader, img_root=rootdir)\n",
        "print(f\"Domain adaptation accuracy Product -> Real World: {acc_da}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XBgT6bYJJCGb",
        "outputId": "7de4dea6-92ce-40b6-b3ac-f88db2155fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.04573, Entropy loss 0.01776, Accuracy 42.88\n",
            "\t Test: CE loss 0.01343, Accuracy 70.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01443, Entropy loss 0.00628, Accuracy 91.88\n",
            "\t Test: CE loss 0.00911, Accuracy 72.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.00750, Entropy loss 0.00577, Accuracy 97.50\n",
            "\t Test: CE loss 0.00739, Accuracy 78.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.00576, Entropy loss 0.00701, Accuracy 98.38\n",
            "\t Test: CE loss 0.00656, Accuracy 80.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00549, Entropy loss 0.00803, Accuracy 98.75\n",
            "\t Test: CE loss 0.00583, Accuracy 82.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00586, Entropy loss 0.00863, Accuracy 99.12\n",
            "\t Test: CE loss 0.00498, Accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00636, Entropy loss 0.00900, Accuracy 99.38\n",
            "\t Test: CE loss 0.00435, Accuracy 85.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00678, Entropy loss 0.00921, Accuracy 99.56\n",
            "\t Test: CE loss 0.00391, Accuracy 87.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00704, Entropy loss 0.00936, Accuracy 99.75\n",
            "\t Test: CE loss 0.00358, Accuracy 88.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00710, Entropy loss 0.00948, Accuracy 99.75\n",
            "\t Test: CE loss 0.00334, Accuracy 88.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00700, Entropy loss 0.00956, Accuracy 99.62\n",
            "\t Test: CE loss 0.00318, Accuracy 89.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00684, Entropy loss 0.00960, Accuracy 99.69\n",
            "\t Test: CE loss 0.00307, Accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00669, Entropy loss 0.00959, Accuracy 99.69\n",
            "\t Test: CE loss 0.00300, Accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00660, Entropy loss 0.00953, Accuracy 99.69\n",
            "\t Test: CE loss 0.00295, Accuracy 89.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00659, Entropy loss 0.00942, Accuracy 99.75\n",
            "\t Test: CE loss 0.00292, Accuracy 89.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Train: CE loss 0.00663, Entropy loss 0.00929, Accuracy 99.81\n",
            "\t Test: CE loss 0.00291, Accuracy 88.75\n",
            "-----------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-f2ac418be93c>\u001b[0m in \u001b[0;36mtraining_step_uda\u001b[0;34m(net, src_data_loader, target_data_loader, optimizer, lam, e, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m       \u001b[0minputs_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m       \u001b[0minputs_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-b190a6d1d4ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc_da\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_uda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrootdir_alessandro\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Domain adaptation accuracy Product -> Real World: {acc_da}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-1ba90a6a1708>\u001b[0m in \u001b[0;36mmain_uda\u001b[0;34m(batch_size, device, epochs, direction, nr_classes, img_root)\u001b[0m\n\u001b[1;32m     32\u001b[0m     train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n\u001b[1;32m     33\u001b[0m                                                         \u001b[0mtarget_data_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                                         optimizer=optimizer, lam=lam, e=e, device=device)\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-f2ac418be93c>\u001b[0m in \u001b[0;36mtraining_step_uda\u001b[0;34m(net, src_data_loader, target_data_loader, optimizer, lam, e, device)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mtarget_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m       \u001b[0minputs_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m       \u001b[0minputs_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# handle PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I;16\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Image'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2 Real World $\\to$ Product\n",
        "#### 6.2.1 Baseline"
      ],
      "metadata": {
        "id": "KpK-evadJDKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_base = main(rw_train_loader, product_test_loader, img_root=rootdir, runs_dir=runs_dir)\n",
        "print(f\"Baseline accuracy Real World -> Product: {acc_base}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SIKC8_SJdXB",
        "outputId": "9e56f8a8-275e-4725-80eb-0ddcc5b8497d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "DataLoaders Done\n",
            "Time to train!\n",
            "==========================BASELINE========================\n",
            "Epoch 0:\n",
            "Training loss: 0.0195840073376894 \n",
            " Training accuracy: 30.8125\n",
            "Epoch 1:\n",
            "Training loss: 0.007453262209892273 \n",
            " Training accuracy: 81.0625\n",
            "Epoch 2:\n",
            "Training loss: 0.003515565562993288 \n",
            " Training accuracy: 91.9375\n",
            "Epoch 3:\n",
            "Training loss: 0.0023773890081793068 \n",
            " Training accuracy: 95.25\n",
            "Epoch 4:\n",
            "Training loss: 0.0019036665000021458 \n",
            " Training accuracy: 96.6875\n",
            "Epoch 5:\n",
            "Training loss: 0.0016059506125748158 \n",
            " Training accuracy: 97.25\n",
            "Epoch 6:\n",
            "Training loss: 0.0014053122233599424 \n",
            " Training accuracy: 97.9375\n",
            "Epoch 7:\n",
            "Training loss: 0.0012587914103642107 \n",
            " Training accuracy: 98.375\n",
            "Epoch 8:\n",
            "Training loss: 0.0011426056222990156 \n",
            " Training accuracy: 98.4375\n",
            "Epoch 9:\n",
            "Training loss: 0.0010492381127551198 \n",
            " Training accuracy: 98.9375\n",
            "Epoch 10:\n",
            "Training loss: 0.000972515526227653 \n",
            " Training accuracy: 99.25\n",
            "Epoch 11:\n",
            "Training loss: 0.0009080296196043491 \n",
            " Training accuracy: 99.375\n",
            "Epoch 12:\n",
            "Training loss: 0.0008529889723286033 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 13:\n",
            "Training loss: 0.0008054186962544918 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 14:\n",
            "Training loss: 0.0007638664171099662 \n",
            " Training accuracy: 99.6875\n",
            "Baseline accuracy Real World -> Product: 91.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.2 Upper bound"
      ],
      "metadata": {
        "id": "VqI-Bp7fJW4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_upperbound = main(product_train_loader, product_test_loader, runs_dir=runs_dir)\n",
        "print(f\"Upper Bound accuracy Real World -> Product: {acc_upperbound}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH-YVGnLJd5p",
        "outputId": "cf178805-7070-4139-d000-4b79a303f645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.01809109877794981 \n",
            " Training accuracy: 40.4375\n",
            "Epoch 1:\n",
            "Training loss: 0.004270306602120399 \n",
            " Training accuracy: 91.75\n",
            "Epoch 2:\n",
            "Training loss: 0.0016956347646191717 \n",
            " Training accuracy: 96.1875\n",
            "Epoch 3:\n",
            "Training loss: 0.0011931087728589774 \n",
            " Training accuracy: 97.4375\n",
            "Epoch 4:\n",
            "Training loss: 0.0009483035514131189 \n",
            " Training accuracy: 97.9375\n",
            "Epoch 5:\n",
            "Training loss: 0.000811921963468194 \n",
            " Training accuracy: 98.8125\n",
            "Epoch 6:\n",
            "Training loss: 0.000718159805983305 \n",
            " Training accuracy: 99.3125\n",
            "Epoch 7:\n",
            "Training loss: 0.0006481324951164425 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 8:\n",
            "Training loss: 0.0005935926060192287 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 9:\n",
            "Training loss: 0.000549595607444644 \n",
            " Training accuracy: 99.5\n",
            "Epoch 10:\n",
            "Training loss: 0.0005132094933651388 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 11:\n",
            "Training loss: 0.000482511242153123 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 12:\n",
            "Training loss: 0.00045620030490681527 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 13:\n",
            "Training loss: 0.00043336430098861456 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 14:\n",
            "Training loss: 0.0004133293218910694 \n",
            " Training accuracy: 99.625\n",
            "Upper Bound accuracy Real World -> Product: 96.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6.2.3 Domain Adaptation"
      ],
      "metadata": {
        "id": "-aSEXfLRJbs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_da = main_uda(rw_train_loader, product_train_loader, product_test_loader, img_root=rootdir)\n",
        "print(f\"Domain adaptation accuracy Product -> Real World: {acc_da}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "defOK8YbJjaZ",
        "outputId": "041b30df-47f1-4d29-8406-d80c792fdb02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders Done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05249, Entropy loss 0.02102, Accuracy 22.38\n",
            "\t Test: CE loss 0.01339, Accuracy 75.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02471, Entropy loss 0.01514, Accuracy 77.75\n",
            "\t Test: CE loss 0.00612, Accuracy 86.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01413, Entropy loss 0.01249, Accuracy 90.75\n",
            "\t Test: CE loss 0.00305, Accuracy 92.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01069, Entropy loss 0.01202, Accuracy 93.69\n",
            "\t Test: CE loss 0.00251, Accuracy 92.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00963, Entropy loss 0.01180, Accuracy 95.44\n",
            "\t Test: CE loss 0.00204, Accuracy 92.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00914, Entropy loss 0.01152, Accuracy 96.50\n",
            "\t Test: CE loss 0.00179, Accuracy 93.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00884, Entropy loss 0.01122, Accuracy 96.69\n",
            "\t Test: CE loss 0.00166, Accuracy 93.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00858, Entropy loss 0.01092, Accuracy 97.19\n",
            "\t Test: CE loss 0.00155, Accuracy 94.25\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00834, Entropy loss 0.01065, Accuracy 97.69\n",
            "\t Test: CE loss 0.00148, Accuracy 94.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00814, Entropy loss 0.01041, Accuracy 98.19\n",
            "\t Test: CE loss 0.00142, Accuracy 94.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00798, Entropy loss 0.01019, Accuracy 98.44\n",
            "\t Test: CE loss 0.00138, Accuracy 94.50\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00783, Entropy loss 0.01000, Accuracy 98.56\n",
            "\t Test: CE loss 0.00135, Accuracy 94.75\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00771, Entropy loss 0.00984, Accuracy 98.62\n",
            "\t Test: CE loss 0.00132, Accuracy 95.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00760, Entropy loss 0.00969, Accuracy 98.88\n",
            "\t Test: CE loss 0.00130, Accuracy 95.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00750, Entropy loss 0.00956, Accuracy 98.94\n",
            "\t Test: CE loss 0.00128, Accuracy 95.00\n",
            "-----------------------------------------------------\n",
            "Domain adaptation accuracy Product -> Real World: 95.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Testing statistical significance of the result\n",
        "It's easy to see from the accuracies above that the domain adaptation method outperforms the baseline. But we want to be sure that the result is statistically significant.\n",
        "In order to do that, we are going to follow the method proposed by the following paper:\n",
        "[Approximate Statistical Tests for Comparing\n",
        "Supervised Classification Learning Algorithms](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.37.3325&rep=rep1&type=pdf). To be precise (Since the paper shows more methods), we are referring to the method proposed in section 3.5: **\"The 5x2cv paired t test\"**"
      ],
      "metadata": {
        "id": "k-MeW-MjTSl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stat\n",
        "from functools import partial\n",
        "\n",
        "\n",
        "def student_test(model1, model2, replications:int, root_dir:str, random_sate_list:List[int], alpha:float, degree_of_freedom: int, batch_size=BATCH_SIZE) -> Tuple[int, str]:\n",
        "  ''' Function that performes T-test of two models\n",
        "    Params:\n",
        "    ------\n",
        "      model1:\n",
        "        first model to compare\n",
        "      model2:\n",
        "        second model to compare\n",
        "      replications: int\n",
        "        number of replications of 2-fold cross-validation \n",
        "      root_dir: str\n",
        "        path to adaptiope_small\n",
        "      random_state_list: list(int)\n",
        "        list of random states when performing 5 times the dataset split\n",
        "      aplha: float\n",
        "        reference value to enstablish statistical significance\n",
        "    Return:\n",
        "    ------\n",
        "      tuple of p-value and string whether statistical significan or not \n",
        "  '''\n",
        "\n",
        "  p1s = []\n",
        "  p2s = []\n",
        "  variances = []\n",
        "  for rep in range(replications):\n",
        "    source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size=batch_size, root_dir=root_dir, random_state=random_sate_list[rep], test_split=0.5)\n",
        "    tst_acc_model1 = model1(source_train_loader, target_test_loader)\n",
        "    tst_acc_model2 = model2(source_train_loader, target_train_loader, target_test_loader)\n",
        "    p1 = tst_acc_model1 - tst_acc_model2\n",
        "    tst_acc_model1 = model1(source_test_loader, target_train_loader)\n",
        "    tst_acc_model2 = model2(source_test_loader, target_test_loader, target_train_loader)\n",
        "    p2 = tst_acc_model1 - tst_acc_model2\n",
        "    if rep == 1:\n",
        "      # to save difference of very first repetition\n",
        "      p11 = p1\n",
        "      p21 = p2\n",
        "  \n",
        "    p_mean = (p1+p2)/2\n",
        "    # s^2 = (p1-p_mean)^2 + (p2-p_mean)^2\n",
        "    variance_i = (p1 - p_mean)**2 + (p2 - p_mean)**2\n",
        "    variances.append(variance_i)\n",
        "  \n",
        "  variances = np.array(variances)\n",
        "  t = p11 / (math.sqrt(np.mean(variances)))\n",
        "  p_value = stat.t.sf(abs(t), df=degree_of_freedom)\n",
        "\n",
        "  if p_value < alpha:\n",
        "    string = 'Null hypothesis rejected. Significance difference in performance between models.'\n",
        "  else :\n",
        "    string = 'No singificant difference.'\n",
        "\n",
        "  return p_value, string"
      ],
      "metadata": {
        "id": "9TBkn_ChucIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p_value, message = student_test(model1 = partial(main), model2 = partial(main_uda), replications=5, root_dir=rootdir, random_sate_list=[91, 11, 57, 822, 19], alpha=0.05, degree_of_freedom=5)\n",
        "print(f\"(p-value, message) : {p_value, message}\")"
      ],
      "metadata": {
        "id": "tnR0SpwuyDqp",
        "outputId": "b9f6a1b5-d99e-41da-ccd6-b8f12ac91cfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.022471324920654297 \n",
            " Training accuracy: 17.5\n",
            "Epoch 1:\n",
            "Training loss: 0.009985714435577393 \n",
            " Training accuracy: 83.89999999999999\n",
            "Epoch 2:\n",
            "Training loss: 0.00423361474275589 \n",
            " Training accuracy: 95.39999999999999\n",
            "Epoch 3:\n",
            "Training loss: 0.0025652790963649748 \n",
            " Training accuracy: 96.7\n",
            "Epoch 4:\n",
            "Training loss: 0.0018961628526449204 \n",
            " Training accuracy: 97.1\n",
            "Epoch 5:\n",
            "Training loss: 0.0015410507023334503 \n",
            " Training accuracy: 97.5\n",
            "Epoch 6:\n",
            "Training loss: 0.001326120674610138 \n",
            " Training accuracy: 97.8\n",
            "Epoch 7:\n",
            "Training loss: 0.0011817959696054459 \n",
            " Training accuracy: 98.1\n",
            "Epoch 8:\n",
            "Training loss: 0.0010747862160205841 \n",
            " Training accuracy: 98.7\n",
            "Epoch 9:\n",
            "Training loss: 0.0009900044724345208 \n",
            " Training accuracy: 99.0\n",
            "Epoch 10:\n",
            "Training loss: 0.0009205811321735382 \n",
            " Training accuracy: 99.1\n",
            "Epoch 11:\n",
            "Training loss: 0.0008625919073820114 \n",
            " Training accuracy: 99.4\n",
            "Epoch 12:\n",
            "Training loss: 0.0008132843896746636 \n",
            " Training accuracy: 99.5\n",
            "Epoch 13:\n",
            "Training loss: 0.0007706941366195679 \n",
            " Training accuracy: 99.5\n",
            "Epoch 14:\n",
            "Training loss: 0.0007334411591291427 \n",
            " Training accuracy: 99.6\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05407, Entropy loss 0.02170, Accuracy 24.80\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02768, Entropy loss 0.01781, Accuracy 84.20\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01525, Entropy loss 0.01705, Accuracy 95.00\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01108, Entropy loss 0.01771, Accuracy 96.30\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00938, Entropy loss 0.01797, Accuracy 97.00\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00858, Entropy loss 0.01764, Accuracy 97.40\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00818, Entropy loss 0.01705, Accuracy 97.70\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00799, Entropy loss 0.01634, Accuracy 98.20\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00791, Entropy loss 0.01564, Accuracy 98.50\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01499, Accuracy 98.70\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01442, Accuracy 98.80\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01392, Accuracy 98.80\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01350, Accuracy 99.00\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00785, Entropy loss 0.01313, Accuracy 99.20\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00780, Entropy loss 0.01282, Accuracy 99.40\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.020993451595306398 \n",
            " Training accuracy: 23.9\n",
            "Epoch 1:\n",
            "Training loss: 0.009223085343837738 \n",
            " Training accuracy: 86.8\n",
            "Epoch 2:\n",
            "Training loss: 0.0038619479537010193 \n",
            " Training accuracy: 96.1\n",
            "Epoch 3:\n",
            "Training loss: 0.002322348102927208 \n",
            " Training accuracy: 97.39999999999999\n",
            "Epoch 4:\n",
            "Training loss: 0.0017248482406139374 \n",
            " Training accuracy: 98.4\n",
            "Epoch 5:\n",
            "Training loss: 0.0013983014672994613 \n",
            " Training accuracy: 98.5\n",
            "Epoch 6:\n",
            "Training loss: 0.0011954587697982789 \n",
            " Training accuracy: 98.7\n",
            "Epoch 7:\n",
            "Training loss: 0.001057307168841362 \n",
            " Training accuracy: 98.9\n",
            "Epoch 8:\n",
            "Training loss: 0.0009562587216496468 \n",
            " Training accuracy: 99.1\n",
            "Epoch 9:\n",
            "Training loss: 0.0008777924999594689 \n",
            " Training accuracy: 99.3\n",
            "Epoch 10:\n",
            "Training loss: 0.0008144510611891746 \n",
            " Training accuracy: 99.4\n",
            "Epoch 11:\n",
            "Training loss: 0.0007619165927171708 \n",
            " Training accuracy: 99.5\n",
            "Epoch 12:\n",
            "Training loss: 0.0007175110280513763 \n",
            " Training accuracy: 99.6\n",
            "Epoch 13:\n",
            "Training loss: 0.0006794096529483796 \n",
            " Training accuracy: 99.7\n",
            "Epoch 14:\n",
            "Training loss: 0.0006463029161095619 \n",
            " Training accuracy: 99.7\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05373, Entropy loss 0.02150, Accuracy 19.50\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02792, Entropy loss 0.01771, Accuracy 85.40\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01520, Entropy loss 0.01692, Accuracy 95.20\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01104, Entropy loss 0.01768, Accuracy 96.40\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00929, Entropy loss 0.01798, Accuracy 97.30\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00851, Entropy loss 0.01768, Accuracy 97.90\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00818, Entropy loss 0.01710, Accuracy 98.10\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00804, Entropy loss 0.01639, Accuracy 98.20\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00797, Entropy loss 0.01571, Accuracy 98.40\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00794, Entropy loss 0.01510, Accuracy 98.60\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00791, Entropy loss 0.01454, Accuracy 98.70\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00787, Entropy loss 0.01406, Accuracy 99.10\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00783, Entropy loss 0.01364, Accuracy 99.20\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00778, Entropy loss 0.01328, Accuracy 99.20\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00773, Entropy loss 0.01295, Accuracy 99.20\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.021898447036743166 \n",
            " Training accuracy: 21.5\n",
            "Epoch 1:\n",
            "Training loss: 0.009730389297008514 \n",
            " Training accuracy: 84.1\n",
            "Epoch 2:\n",
            "Training loss: 0.004183801084756851 \n",
            " Training accuracy: 95.5\n",
            "Epoch 3:\n",
            "Training loss: 0.0025077418684959414 \n",
            " Training accuracy: 96.7\n",
            "Epoch 4:\n",
            "Training loss: 0.0018614105880260467 \n",
            " Training accuracy: 97.39999999999999\n",
            "Epoch 5:\n",
            "Training loss: 0.0015229083448648453 \n",
            " Training accuracy: 97.7\n",
            "Epoch 6:\n",
            "Training loss: 0.0013090893775224686 \n",
            " Training accuracy: 98.0\n",
            "Epoch 7:\n",
            "Training loss: 0.001162295639514923 \n",
            " Training accuracy: 98.3\n",
            "Epoch 8:\n",
            "Training loss: 0.0010537796616554261 \n",
            " Training accuracy: 98.6\n",
            "Epoch 9:\n",
            "Training loss: 0.0009685991629958153 \n",
            " Training accuracy: 98.8\n",
            "Epoch 10:\n",
            "Training loss: 0.000899406336247921 \n",
            " Training accuracy: 99.0\n",
            "Epoch 11:\n",
            "Training loss: 0.0008418297246098518 \n",
            " Training accuracy: 99.1\n",
            "Epoch 12:\n",
            "Training loss: 0.0007930397093296051 \n",
            " Training accuracy: 99.2\n",
            "Epoch 13:\n",
            "Training loss: 0.0007510315626859665 \n",
            " Training accuracy: 99.4\n",
            "Epoch 14:\n",
            "Training loss: 0.0007144041061401367 \n",
            " Training accuracy: 99.4\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05320, Entropy loss 0.02130, Accuracy 24.00\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02750, Entropy loss 0.01774, Accuracy 86.10\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01508, Entropy loss 0.01723, Accuracy 94.60\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01092, Entropy loss 0.01807, Accuracy 96.60\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00921, Entropy loss 0.01848, Accuracy 97.30\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00843, Entropy loss 0.01823, Accuracy 97.80\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00807, Entropy loss 0.01762, Accuracy 98.20\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00793, Entropy loss 0.01685, Accuracy 98.30\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01608, Accuracy 98.50\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01538, Accuracy 98.50\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00789, Entropy loss 0.01477, Accuracy 98.60\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00790, Entropy loss 0.01424, Accuracy 98.60\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00790, Entropy loss 0.01378, Accuracy 98.80\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01339, Accuracy 98.80\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00785, Entropy loss 0.01305, Accuracy 98.90\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.02091357672214508 \n",
            " Training accuracy: 26.0\n",
            "Epoch 1:\n",
            "Training loss: 0.009373283624649047 \n",
            " Training accuracy: 83.5\n",
            "Epoch 2:\n",
            "Training loss: 0.00390545654296875 \n",
            " Training accuracy: 94.8\n",
            "Epoch 3:\n",
            "Training loss: 0.002348865240812302 \n",
            " Training accuracy: 96.5\n",
            "Epoch 4:\n",
            "Training loss: 0.0017383954375982285 \n",
            " Training accuracy: 97.6\n",
            "Epoch 5:\n",
            "Training loss: 0.0014113855063915253 \n",
            " Training accuracy: 98.3\n",
            "Epoch 6:\n",
            "Training loss: 0.0012117296159267425 \n",
            " Training accuracy: 98.5\n",
            "Epoch 7:\n",
            "Training loss: 0.0010763514637947083 \n",
            " Training accuracy: 98.9\n",
            "Epoch 8:\n",
            "Training loss: 0.0009761969968676567 \n",
            " Training accuracy: 99.2\n",
            "Epoch 9:\n",
            "Training loss: 0.0008976681530475616 \n",
            " Training accuracy: 99.3\n",
            "Epoch 10:\n",
            "Training loss: 0.0008338175192475318 \n",
            " Training accuracy: 99.4\n",
            "Epoch 11:\n",
            "Training loss: 0.0007805939391255379 \n",
            " Training accuracy: 99.4\n",
            "Epoch 12:\n",
            "Training loss: 0.0007354584783315659 \n",
            " Training accuracy: 99.5\n",
            "Epoch 13:\n",
            "Training loss: 0.0006965545937418938 \n",
            " Training accuracy: 99.7\n",
            "Epoch 14:\n",
            "Training loss: 0.0006625823453068734 \n",
            " Training accuracy: 99.8\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05525, Entropy loss 0.02206, Accuracy 20.70\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02829, Entropy loss 0.01790, Accuracy 87.00\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01544, Entropy loss 0.01705, Accuracy 95.40\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01102, Entropy loss 0.01767, Accuracy 96.50\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00919, Entropy loss 0.01773, Accuracy 97.50\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00834, Entropy loss 0.01753, Accuracy 98.10\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00793, Entropy loss 0.01706, Accuracy 98.40\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00773, Entropy loss 0.01643, Accuracy 98.60\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00765, Entropy loss 0.01582, Accuracy 98.90\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00763, Entropy loss 0.01523, Accuracy 98.90\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00763, Entropy loss 0.01469, Accuracy 99.00\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00763, Entropy loss 0.01422, Accuracy 99.10\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00762, Entropy loss 0.01381, Accuracy 99.10\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00759, Entropy loss 0.01345, Accuracy 99.30\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00756, Entropy loss 0.01313, Accuracy 99.30\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.021503018379211425 \n",
            " Training accuracy: 21.9\n",
            "Epoch 1:\n",
            "Training loss: 0.009512508511543274 \n",
            " Training accuracy: 85.0\n",
            "Epoch 2:\n",
            "Training loss: 0.0041829623878002166 \n",
            " Training accuracy: 95.0\n",
            "Epoch 3:\n",
            "Training loss: 0.0025044180154800413 \n",
            " Training accuracy: 96.6\n",
            "Epoch 4:\n",
            "Training loss: 0.001833905130624771 \n",
            " Training accuracy: 97.6\n",
            "Epoch 5:\n",
            "Training loss: 0.00149913689494133 \n",
            " Training accuracy: 97.89999999999999\n",
            "Epoch 6:\n",
            "Training loss: 0.0012923706024885178 \n",
            " Training accuracy: 98.2\n",
            "Epoch 7:\n",
            "Training loss: 0.001150378681719303 \n",
            " Training accuracy: 98.5\n",
            "Epoch 8:\n",
            "Training loss: 0.0010455473437905312 \n",
            " Training accuracy: 98.7\n",
            "Epoch 9:\n",
            "Training loss: 0.0009636260271072387 \n",
            " Training accuracy: 98.9\n",
            "Epoch 10:\n",
            "Training loss: 0.0008970872834324837 \n",
            " Training accuracy: 99.1\n",
            "Epoch 11:\n",
            "Training loss: 0.0008414998576045036 \n",
            " Training accuracy: 99.3\n",
            "Epoch 12:\n",
            "Training loss: 0.0007940990999341011 \n",
            " Training accuracy: 99.4\n",
            "Epoch 13:\n",
            "Training loss: 0.0007530589550733566 \n",
            " Training accuracy: 99.4\n",
            "Epoch 14:\n",
            "Training loss: 0.0007170951813459396 \n",
            " Training accuracy: 99.6\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05487, Entropy loss 0.02216, Accuracy 19.30\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02840, Entropy loss 0.01825, Accuracy 85.50\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01542, Entropy loss 0.01738, Accuracy 94.60\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01097, Entropy loss 0.01797, Accuracy 97.00\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00923, Entropy loss 0.01813, Accuracy 97.60\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00847, Entropy loss 0.01776, Accuracy 97.90\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00813, Entropy loss 0.01717, Accuracy 98.00\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00802, Entropy loss 0.01645, Accuracy 98.20\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00799, Entropy loss 0.01575, Accuracy 98.30\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00799, Entropy loss 0.01513, Accuracy 98.60\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00798, Entropy loss 0.01457, Accuracy 98.80\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00797, Entropy loss 0.01408, Accuracy 98.80\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00795, Entropy loss 0.01366, Accuracy 98.90\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00793, Entropy loss 0.01329, Accuracy 98.90\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00789, Entropy loss 0.01297, Accuracy 98.90\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.021634153842926024 \n",
            " Training accuracy: 22.2\n",
            "Epoch 1:\n",
            "Training loss: 0.009587165296077728 \n",
            " Training accuracy: 86.0\n",
            "Epoch 2:\n",
            "Training loss: 0.004098121076822281 \n",
            " Training accuracy: 94.8\n",
            "Epoch 3:\n",
            "Training loss: 0.002507189705967903 \n",
            " Training accuracy: 96.7\n",
            "Epoch 4:\n",
            "Training loss: 0.0018598485440015793 \n",
            " Training accuracy: 97.6\n",
            "Epoch 5:\n",
            "Training loss: 0.0015145235285162926 \n",
            " Training accuracy: 98.1\n",
            "Epoch 6:\n",
            "Training loss: 0.0013013944327831268 \n",
            " Training accuracy: 98.3\n",
            "Epoch 7:\n",
            "Training loss: 0.0011554981246590615 \n",
            " Training accuracy: 98.4\n",
            "Epoch 8:\n",
            "Training loss: 0.0010471796467900276 \n",
            " Training accuracy: 98.8\n",
            "Epoch 9:\n",
            "Training loss: 0.0009620782807469368 \n",
            " Training accuracy: 99.3\n",
            "Epoch 10:\n",
            "Training loss: 0.0008928915858268738 \n",
            " Training accuracy: 99.3\n",
            "Epoch 11:\n",
            "Training loss: 0.0008352338969707489 \n",
            " Training accuracy: 99.3\n",
            "Epoch 12:\n",
            "Training loss: 0.0007863340750336647 \n",
            " Training accuracy: 99.5\n",
            "Epoch 13:\n",
            "Training loss: 0.0007442515827715397 \n",
            " Training accuracy: 99.6\n",
            "Epoch 14:\n",
            "Training loss: 0.0007075825817883015 \n",
            " Training accuracy: 99.6\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05324, Entropy loss 0.02120, Accuracy 23.90\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02693, Entropy loss 0.01733, Accuracy 84.70\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01476, Entropy loss 0.01692, Accuracy 95.40\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01078, Entropy loss 0.01782, Accuracy 95.90\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00917, Entropy loss 0.01811, Accuracy 96.60\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00845, Entropy loss 0.01782, Accuracy 97.60\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00813, Entropy loss 0.01720, Accuracy 97.70\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00797, Entropy loss 0.01646, Accuracy 98.10\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00790, Entropy loss 0.01574, Accuracy 98.40\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00786, Entropy loss 0.01508, Accuracy 98.80\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00783, Entropy loss 0.01451, Accuracy 98.90\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00780, Entropy loss 0.01401, Accuracy 98.90\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00777, Entropy loss 0.01359, Accuracy 99.20\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00773, Entropy loss 0.01323, Accuracy 99.30\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00769, Entropy loss 0.01291, Accuracy 99.40\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.022207922458648683 \n",
            " Training accuracy: 19.5\n",
            "Epoch 1:\n",
            "Training loss: 0.009996236503124237 \n",
            " Training accuracy: 84.6\n",
            "Epoch 2:\n",
            "Training loss: 0.004217775523662567 \n",
            " Training accuracy: 95.5\n",
            "Epoch 3:\n",
            "Training loss: 0.0025313416719436647 \n",
            " Training accuracy: 96.8\n",
            "Epoch 4:\n",
            "Training loss: 0.0018460927009582519 \n",
            " Training accuracy: 97.7\n",
            "Epoch 5:\n",
            "Training loss: 0.0014917254596948624 \n",
            " Training accuracy: 98.3\n",
            "Epoch 6:\n",
            "Training loss: 0.0012734952494502067 \n",
            " Training accuracy: 98.9\n",
            "Epoch 7:\n",
            "Training loss: 0.0011259032264351846 \n",
            " Training accuracy: 98.9\n",
            "Epoch 8:\n",
            "Training loss: 0.0010170287564396857 \n",
            " Training accuracy: 98.9\n",
            "Epoch 9:\n",
            "Training loss: 0.0009317603036761284 \n",
            " Training accuracy: 99.5\n",
            "Epoch 10:\n",
            "Training loss: 0.0008626385107636451 \n",
            " Training accuracy: 99.6\n",
            "Epoch 11:\n",
            "Training loss: 0.0008052009344100953 \n",
            " Training accuracy: 99.6\n",
            "Epoch 12:\n",
            "Training loss: 0.0007565862461924553 \n",
            " Training accuracy: 99.6\n",
            "Epoch 13:\n",
            "Training loss: 0.000714815080165863 \n",
            " Training accuracy: 99.6\n",
            "Epoch 14:\n",
            "Training loss: 0.0006785206943750381 \n",
            " Training accuracy: 99.8\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05399, Entropy loss 0.02158, Accuracy 21.80\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02809, Entropy loss 0.01789, Accuracy 83.40\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01543, Entropy loss 0.01715, Accuracy 94.60\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01124, Entropy loss 0.01773, Accuracy 97.20\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00945, Entropy loss 0.01802, Accuracy 97.70\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00860, Entropy loss 0.01766, Accuracy 97.90\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00820, Entropy loss 0.01703, Accuracy 98.30\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00802, Entropy loss 0.01631, Accuracy 98.50\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00797, Entropy loss 0.01559, Accuracy 98.80\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00796, Entropy loss 0.01495, Accuracy 98.90\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00794, Entropy loss 0.01440, Accuracy 99.00\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00792, Entropy loss 0.01392, Accuracy 99.00\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00787, Entropy loss 0.01351, Accuracy 99.10\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00782, Entropy loss 0.01316, Accuracy 99.10\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00776, Entropy loss 0.01285, Accuracy 99.10\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.021812050580978393 \n",
            " Training accuracy: 24.7\n",
            "Epoch 1:\n",
            "Training loss: 0.009748446047306061 \n",
            " Training accuracy: 80.0\n",
            "Epoch 2:\n",
            "Training loss: 0.0039949239790439605 \n",
            " Training accuracy: 94.89999999999999\n",
            "Epoch 3:\n",
            "Training loss: 0.0023547677248716356 \n",
            " Training accuracy: 96.89999999999999\n",
            "Epoch 4:\n",
            "Training loss: 0.0017077910900115967 \n",
            " Training accuracy: 97.8\n",
            "Epoch 5:\n",
            "Training loss: 0.001379174992442131 \n",
            " Training accuracy: 98.3\n",
            "Epoch 6:\n",
            "Training loss: 0.0011818609833717346 \n",
            " Training accuracy: 98.3\n",
            "Epoch 7:\n",
            "Training loss: 0.0010495433881878853 \n",
            " Training accuracy: 98.5\n",
            "Epoch 8:\n",
            "Training loss: 0.0009534948170185089 \n",
            " Training accuracy: 98.7\n",
            "Epoch 9:\n",
            "Training loss: 0.0008778940364718437 \n",
            " Training accuracy: 98.9\n",
            "Epoch 10:\n",
            "Training loss: 0.0008164767697453499 \n",
            " Training accuracy: 99.0\n",
            "Epoch 11:\n",
            "Training loss: 0.0007655174508690835 \n",
            " Training accuracy: 99.1\n",
            "Epoch 12:\n",
            "Training loss: 0.0007223695144057274 \n",
            " Training accuracy: 99.2\n",
            "Epoch 13:\n",
            "Training loss: 0.0006851900592446328 \n",
            " Training accuracy: 99.5\n",
            "Epoch 14:\n",
            "Training loss: 0.000652734525501728 \n",
            " Training accuracy: 99.6\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05369, Entropy loss 0.02147, Accuracy 20.50\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02762, Entropy loss 0.01764, Accuracy 84.10\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01499, Entropy loss 0.01693, Accuracy 95.30\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01072, Entropy loss 0.01776, Accuracy 97.10\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00901, Entropy loss 0.01812, Accuracy 97.60\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00824, Entropy loss 0.01794, Accuracy 98.20\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00789, Entropy loss 0.01737, Accuracy 98.40\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00777, Entropy loss 0.01664, Accuracy 98.60\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00778, Entropy loss 0.01589, Accuracy 98.60\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00784, Entropy loss 0.01519, Accuracy 98.70\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00790, Entropy loss 0.01457, Accuracy 98.60\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00793, Entropy loss 0.01404, Accuracy 98.70\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00792, Entropy loss 0.01359, Accuracy 99.00\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01321, Accuracy 99.00\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00782, Entropy loss 0.01288, Accuracy 99.10\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.020995357036590577 \n",
            " Training accuracy: 23.799999999999997\n",
            "Epoch 1:\n",
            "Training loss: 0.009214461445808411 \n",
            " Training accuracy: 87.5\n",
            "Epoch 2:\n",
            "Training loss: 0.004031994253396988 \n",
            " Training accuracy: 96.0\n",
            "Epoch 3:\n",
            "Training loss: 0.0024524467289447783 \n",
            " Training accuracy: 96.89999999999999\n",
            "Epoch 4:\n",
            "Training loss: 0.0018279396891593934 \n",
            " Training accuracy: 97.39999999999999\n",
            "Epoch 5:\n",
            "Training loss: 0.0015022050142288208 \n",
            " Training accuracy: 97.89999999999999\n",
            "Epoch 6:\n",
            "Training loss: 0.0013009141981601715 \n",
            " Training accuracy: 98.0\n",
            "Epoch 7:\n",
            "Training loss: 0.0011634627655148506 \n",
            " Training accuracy: 98.2\n",
            "Epoch 8:\n",
            "Training loss: 0.0010614997074007988 \n",
            " Training accuracy: 98.4\n",
            "Epoch 9:\n",
            "Training loss: 0.0009813180416822433 \n",
            " Training accuracy: 98.8\n",
            "Epoch 10:\n",
            "Training loss: 0.00091583351790905 \n",
            " Training accuracy: 98.9\n",
            "Epoch 11:\n",
            "Training loss: 0.0008610027432441711 \n",
            " Training accuracy: 98.9\n",
            "Epoch 12:\n",
            "Training loss: 0.0008142004385590553 \n",
            " Training accuracy: 99.1\n",
            "Epoch 13:\n",
            "Training loss: 0.0007736254930496215 \n",
            " Training accuracy: 99.1\n",
            "Epoch 14:\n",
            "Training loss: 0.0007380151823163033 \n",
            " Training accuracy: 99.3\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05361, Entropy loss 0.02149, Accuracy 23.40\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02764, Entropy loss 0.01769, Accuracy 85.70\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01546, Entropy loss 0.01710, Accuracy 94.20\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01123, Entropy loss 0.01792, Accuracy 95.70\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00952, Entropy loss 0.01820, Accuracy 96.80\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00874, Entropy loss 0.01787, Accuracy 97.40\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00842, Entropy loss 0.01726, Accuracy 97.70\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00828, Entropy loss 0.01648, Accuracy 97.70\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00820, Entropy loss 0.01573, Accuracy 98.20\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00815, Entropy loss 0.01507, Accuracy 98.50\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00811, Entropy loss 0.01447, Accuracy 98.70\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00808, Entropy loss 0.01396, Accuracy 98.80\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00804, Entropy loss 0.01351, Accuracy 98.80\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00800, Entropy loss 0.01313, Accuracy 98.90\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00794, Entropy loss 0.01280, Accuracy 98.90\n",
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.0224247043132782 \n",
            " Training accuracy: 19.7\n",
            "Epoch 1:\n",
            "Training loss: 0.009896339952945709 \n",
            " Training accuracy: 85.1\n",
            "Epoch 2:\n",
            "Training loss: 0.004230942279100418 \n",
            " Training accuracy: 95.6\n",
            "Epoch 3:\n",
            "Training loss: 0.0024665784388780594 \n",
            " Training accuracy: 97.1\n",
            "Epoch 4:\n",
            "Training loss: 0.0017974893003702163 \n",
            " Training accuracy: 97.89999999999999\n",
            "Epoch 5:\n",
            "Training loss: 0.0014515246450901031 \n",
            " Training accuracy: 98.3\n",
            "Epoch 6:\n",
            "Training loss: 0.001238930895924568 \n",
            " Training accuracy: 98.6\n",
            "Epoch 7:\n",
            "Training loss: 0.0010941692888736725 \n",
            " Training accuracy: 99.0\n",
            "Epoch 8:\n",
            "Training loss: 0.0009879190474748611 \n",
            " Training accuracy: 99.2\n",
            "Epoch 9:\n",
            "Training loss: 0.0009056922569870948 \n",
            " Training accuracy: 99.2\n",
            "Epoch 10:\n",
            "Training loss: 0.0008394551575183868 \n",
            " Training accuracy: 99.5\n",
            "Epoch 11:\n",
            "Training loss: 0.0007846506312489509 \n",
            " Training accuracy: 99.7\n",
            "Epoch 12:\n",
            "Training loss: 0.0007383477836847305 \n",
            " Training accuracy: 99.7\n",
            "Epoch 13:\n",
            "Training loss: 0.0006986153945326805 \n",
            " Training accuracy: 99.7\n",
            "Epoch 14:\n",
            "Training loss: 0.0006640553697943687 \n",
            " Training accuracy: 99.7\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.05456, Entropy loss 0.02180, Accuracy 21.80\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.02816, Entropy loss 0.01800, Accuracy 82.40\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01515, Entropy loss 0.01711, Accuracy 94.60\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01088, Entropy loss 0.01778, Accuracy 96.50\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00914, Entropy loss 0.01809, Accuracy 97.20\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00831, Entropy loss 0.01772, Accuracy 97.80\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00793, Entropy loss 0.01714, Accuracy 98.40\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00776, Entropy loss 0.01643, Accuracy 98.60\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00771, Entropy loss 0.01572, Accuracy 98.90\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00770, Entropy loss 0.01509, Accuracy 99.00\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00770, Entropy loss 0.01454, Accuracy 99.20\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00769, Entropy loss 0.01405, Accuracy 99.30\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00767, Entropy loss 0.01363, Accuracy 99.30\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00763, Entropy loss 0.01328, Accuracy 99.50\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00758, Entropy loss 0.01297, Accuracy 99.50\n",
            "(p-value, message) : (0.012838542567744218, 'Null hypothesis rejected. Significance difference in performance between models.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_accuracies = np.empty(0)\n",
        "uda_accuracies = np.empty(0)\n",
        "\n",
        "# randomly chosen seeds for the dataset split\n",
        "seeds = [91, 11, 57, 822, 19]\n",
        "\n",
        "for seed in seeds:\n",
        "  product_train_loader, product_test_loader, rw_train_loader, rw_test_loader = get_data(128, rootdir, random_state=seed)\n",
        "\n",
        "  base_acc = main(product_train_loader, rw_test_loader, runs_dir=runs_dir)\n",
        "  baseline_accuracies = np.append(baseline_accuracies, base_acc)\n",
        "\n",
        "  uda_acc = main_uda(product_train_loader, rw_train_loader, rw_test_loader, img_root=rootdir)\n",
        "  uda_accuracies = np.append(uda_accuracies, uda_acc)\n",
        "\n",
        "  print(f\"baseline_accuracies: {baseline_accuracies}\")\n",
        "  print(f\"uda accuracies: {uda_accuracies}\")\n"
      ],
      "metadata": {
        "id": "Lqaw6HypVGc_",
        "outputId": "ff9ea39f-bc14-4934-9831-22c9eb835d36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.018003707081079484 \n",
            " Training accuracy: 41.9375\n",
            "Epoch 1:\n",
            "Training loss: 0.004193783309310675 \n",
            " Training accuracy: 91.375\n",
            "Epoch 2:\n",
            "Training loss: 0.0017572841746732592 \n",
            " Training accuracy: 95.9375\n",
            "Epoch 3:\n",
            "Training loss: 0.0012142252689227463 \n",
            " Training accuracy: 97.375\n",
            "Epoch 4:\n",
            "Training loss: 0.0009767045616172255 \n",
            " Training accuracy: 98.125\n",
            "Epoch 5:\n",
            "Training loss: 0.0008371801604516805 \n",
            " Training accuracy: 98.5625\n",
            "Epoch 6:\n",
            "Training loss: 0.0007409020862542093 \n",
            " Training accuracy: 98.75\n",
            "Epoch 7:\n",
            "Training loss: 0.0006697058188728988 \n",
            " Training accuracy: 98.9375\n",
            "Epoch 8:\n",
            "Training loss: 0.0006136374524794519 \n",
            " Training accuracy: 99.25\n",
            "Epoch 9:\n",
            "Training loss: 0.0005680043110623956 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 10:\n",
            "Training loss: 0.0005301959649659693 \n",
            " Training accuracy: 99.5\n",
            "Epoch 11:\n",
            "Training loss: 0.0004983114521019161 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 12:\n",
            "Training loss: 0.00047096268506720664 \n",
            " Training accuracy: 99.625\n",
            "Epoch 13:\n",
            "Training loss: 0.0004472108744084835 \n",
            " Training accuracy: 99.625\n",
            "Epoch 14:\n",
            "Training loss: 0.00042637393227778374 \n",
            " Training accuracy: 99.6875\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.04626, Entropy loss 0.01777, Accuracy 42.44\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01519, Entropy loss 0.01099, Accuracy 92.62\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.00897, Entropy loss 0.01270, Accuracy 96.06\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.00763, Entropy loss 0.01427, Accuracy 97.38\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00766, Entropy loss 0.01439, Accuracy 97.88\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00799, Entropy loss 0.01395, Accuracy 98.00\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00823, Entropy loss 0.01335, Accuracy 98.31\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00829, Entropy loss 0.01281, Accuracy 98.62\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00823, Entropy loss 0.01234, Accuracy 98.81\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00809, Entropy loss 0.01196, Accuracy 98.88\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00792, Entropy loss 0.01163, Accuracy 98.94\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00774, Entropy loss 0.01137, Accuracy 99.00\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00758, Entropy loss 0.01114, Accuracy 99.12\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00743, Entropy loss 0.01095, Accuracy 99.12\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00730, Entropy loss 0.01078, Accuracy 99.19\n",
            "baseline_accuracies: [77.]\n",
            "uda accuracies: [85.25]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.01821212224662304 \n",
            " Training accuracy: 40.25\n",
            "Epoch 1:\n",
            "Training loss: 0.004134849980473519 \n",
            " Training accuracy: 92.875\n",
            "Epoch 2:\n",
            "Training loss: 0.001741063268855214 \n",
            " Training accuracy: 96.5625\n",
            "Epoch 3:\n",
            "Training loss: 0.0012065851641818882 \n",
            " Training accuracy: 97.9375\n",
            "Epoch 4:\n",
            "Training loss: 0.000961151747033 \n",
            " Training accuracy: 98.4375\n",
            "Epoch 5:\n",
            "Training loss: 0.000817558285780251 \n",
            " Training accuracy: 98.75\n",
            "Epoch 6:\n",
            "Training loss: 0.000719412041362375 \n",
            " Training accuracy: 99.0\n",
            "Epoch 7:\n",
            "Training loss: 0.0006469863979145884 \n",
            " Training accuracy: 99.25\n",
            "Epoch 8:\n",
            "Training loss: 0.0005907136318273842 \n",
            " Training accuracy: 99.6875\n",
            "Epoch 9:\n",
            "Training loss: 0.0005454503209330142 \n",
            " Training accuracy: 99.6875\n",
            "Epoch 10:\n",
            "Training loss: 0.0005081417574547231 \n",
            " Training accuracy: 99.6875\n",
            "Epoch 11:\n",
            "Training loss: 0.0004768064571544528 \n",
            " Training accuracy: 99.75\n",
            "Epoch 12:\n",
            "Training loss: 0.00045004433253780006 \n",
            " Training accuracy: 99.8125\n",
            "Epoch 13:\n",
            "Training loss: 0.0004269026312977076 \n",
            " Training accuracy: 99.8125\n",
            "Epoch 14:\n",
            "Training loss: 0.0004066747589968145 \n",
            " Training accuracy: 99.8125\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.04581, Entropy loss 0.01777, Accuracy 44.06\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01477, Entropy loss 0.01088, Accuracy 94.31\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.00875, Entropy loss 0.01264, Accuracy 97.19\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.00747, Entropy loss 0.01412, Accuracy 97.88\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00746, Entropy loss 0.01428, Accuracy 98.31\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00782, Entropy loss 0.01381, Accuracy 98.50\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00809, Entropy loss 0.01323, Accuracy 98.62\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00816, Entropy loss 0.01272, Accuracy 98.75\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00810, Entropy loss 0.01229, Accuracy 98.94\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00797, Entropy loss 0.01193, Accuracy 99.06\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00781, Entropy loss 0.01163, Accuracy 99.12\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00764, Entropy loss 0.01137, Accuracy 99.12\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00746, Entropy loss 0.01116, Accuracy 99.25\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00730, Entropy loss 0.01098, Accuracy 99.25\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00716, Entropy loss 0.01081, Accuracy 99.25\n",
            "baseline_accuracies: [77.  78.5]\n",
            "uda accuracies: [85.25 87.5 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.01774728760123253 \n",
            " Training accuracy: 43.375\n",
            "Epoch 1:\n",
            "Training loss: 0.004058751221746207 \n",
            " Training accuracy: 92.3125\n",
            "Epoch 2:\n",
            "Training loss: 0.0016710284817963838 \n",
            " Training accuracy: 97.0\n",
            "Epoch 3:\n",
            "Training loss: 0.0011533382628113032 \n",
            " Training accuracy: 97.75\n",
            "Epoch 4:\n",
            "Training loss: 0.0009109698189422488 \n",
            " Training accuracy: 98.5\n",
            "Epoch 5:\n",
            "Training loss: 0.0007751479744911194 \n",
            " Training accuracy: 99.25\n",
            "Epoch 6:\n",
            "Training loss: 0.0006809956301003694 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 7:\n",
            "Training loss: 0.0006118861096911133 \n",
            " Training accuracy: 99.6875\n",
            "Epoch 8:\n",
            "Training loss: 0.0005588559992611409 \n",
            " Training accuracy: 99.6875\n",
            "Epoch 9:\n",
            "Training loss: 0.0005162901873700321 \n",
            " Training accuracy: 99.75\n",
            "Epoch 10:\n",
            "Training loss: 0.00048120530555024744 \n",
            " Training accuracy: 99.8125\n",
            "Epoch 11:\n",
            "Training loss: 0.00045175325591117143 \n",
            " Training accuracy: 99.8125\n",
            "Epoch 12:\n",
            "Training loss: 0.0004266248922795057 \n",
            " Training accuracy: 99.8125\n",
            "Epoch 13:\n",
            "Training loss: 0.00040488393511623144 \n",
            " Training accuracy: 99.8125\n",
            "Epoch 14:\n",
            "Training loss: 0.0003858649055473506 \n",
            " Training accuracy: 99.8125\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.04580, Entropy loss 0.01766, Accuracy 42.19\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01479, Entropy loss 0.01084, Accuracy 92.44\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.00860, Entropy loss 0.01258, Accuracy 96.62\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.00725, Entropy loss 0.01408, Accuracy 97.81\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00719, Entropy loss 0.01433, Accuracy 98.12\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00748, Entropy loss 0.01390, Accuracy 98.62\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00775, Entropy loss 0.01332, Accuracy 99.00\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00791, Entropy loss 0.01278, Accuracy 99.06\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00798, Entropy loss 0.01232, Accuracy 99.06\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00795, Entropy loss 0.01194, Accuracy 99.19\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00784, Entropy loss 0.01163, Accuracy 99.31\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00769, Entropy loss 0.01137, Accuracy 99.38\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00754, Entropy loss 0.01115, Accuracy 99.44\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00738, Entropy loss 0.01096, Accuracy 99.56\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00724, Entropy loss 0.01080, Accuracy 99.50\n",
            "baseline_accuracies: [77.   78.5  79.25]\n",
            "uda accuracies: [85.25 87.5  87.  ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.017751939222216608 \n",
            " Training accuracy: 42.3125\n",
            "Epoch 1:\n",
            "Training loss: 0.0040741883590817455 \n",
            " Training accuracy: 93.4375\n",
            "Epoch 2:\n",
            "Training loss: 0.001780945621430874 \n",
            " Training accuracy: 96.3125\n",
            "Epoch 3:\n",
            "Training loss: 0.001222041556611657 \n",
            " Training accuracy: 97.8125\n",
            "Epoch 4:\n",
            "Training loss: 0.0009798221779055893 \n",
            " Training accuracy: 98.5\n",
            "Epoch 5:\n",
            "Training loss: 0.0008412263100035489 \n",
            " Training accuracy: 98.9375\n",
            "Epoch 6:\n",
            "Training loss: 0.0007449926668778062 \n",
            " Training accuracy: 99.1875\n",
            "Epoch 7:\n",
            "Training loss: 0.0006731950002722442 \n",
            " Training accuracy: 99.25\n",
            "Epoch 8:\n",
            "Training loss: 0.000617189051117748 \n",
            " Training accuracy: 99.3125\n",
            "Epoch 9:\n",
            "Training loss: 0.0005718926945701241 \n",
            " Training accuracy: 99.375\n",
            "Epoch 10:\n",
            "Training loss: 0.0005343244899995625 \n",
            " Training accuracy: 99.375\n",
            "Epoch 11:\n",
            "Training loss: 0.0005025446123909205 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 12:\n",
            "Training loss: 0.0004752443241886795 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 13:\n",
            "Training loss: 0.00045152443810366096 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 14:\n",
            "Training loss: 0.00043068315484561025 \n",
            " Training accuracy: 99.75\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.04545, Entropy loss 0.01749, Accuracy 43.88\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01479, Entropy loss 0.01082, Accuracy 93.44\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.00870, Entropy loss 0.01252, Accuracy 96.56\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.00745, Entropy loss 0.01404, Accuracy 97.94\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00745, Entropy loss 0.01418, Accuracy 98.12\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00780, Entropy loss 0.01375, Accuracy 98.62\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00807, Entropy loss 0.01321, Accuracy 98.75\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00818, Entropy loss 0.01270, Accuracy 99.00\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00815, Entropy loss 0.01228, Accuracy 99.06\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00804, Entropy loss 0.01192, Accuracy 99.25\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00789, Entropy loss 0.01162, Accuracy 99.38\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00773, Entropy loss 0.01136, Accuracy 99.38\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00757, Entropy loss 0.01114, Accuracy 99.38\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00744, Entropy loss 0.01094, Accuracy 99.38\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00732, Entropy loss 0.01077, Accuracy 99.38\n",
            "baseline_accuracies: [77.   78.5  79.25 79.75]\n",
            "uda accuracies: [85.25 87.5  87.   87.  ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network Init Done\n",
            "Got Optimizer\n",
            "Got Cost Function\n",
            "Epoch 0:\n",
            "Training loss: 0.018018654584884643 \n",
            " Training accuracy: 42.5625\n",
            "Epoch 1:\n",
            "Training loss: 0.004225632399320603 \n",
            " Training accuracy: 93.6875\n",
            "Epoch 2:\n",
            "Training loss: 0.001737426118925214 \n",
            " Training accuracy: 96.8125\n",
            "Epoch 3:\n",
            "Training loss: 0.0012205179780721664 \n",
            " Training accuracy: 97.5\n",
            "Epoch 4:\n",
            "Training loss: 0.0009836634900420903 \n",
            " Training accuracy: 98.3125\n",
            "Epoch 5:\n",
            "Training loss: 0.0008441854687407613 \n",
            " Training accuracy: 98.6875\n",
            "Epoch 6:\n",
            "Training loss: 0.000747526572085917 \n",
            " Training accuracy: 98.9375\n",
            "Epoch 7:\n",
            "Training loss: 0.0006759040290489793 \n",
            " Training accuracy: 99.0\n",
            "Epoch 8:\n",
            "Training loss: 0.000619657018687576 \n",
            " Training accuracy: 99.125\n",
            "Epoch 9:\n",
            "Training loss: 0.0005737917008809745 \n",
            " Training accuracy: 99.25\n",
            "Epoch 10:\n",
            "Training loss: 0.0005356373894028365 \n",
            " Training accuracy: 99.4375\n",
            "Epoch 11:\n",
            "Training loss: 0.0005033424030989409 \n",
            " Training accuracy: 99.5625\n",
            "Epoch 12:\n",
            "Training loss: 0.00047564606182277203 \n",
            " Training accuracy: 99.6875\n",
            "Epoch 13:\n",
            "Training loss: 0.0004515897179953754 \n",
            " Training accuracy: 99.6875\n",
            "Epoch 14:\n",
            "Training loss: 0.00043046247912570835 \n",
            " Training accuracy: 99.6875\n",
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.04576, Entropy loss 0.01761, Accuracy 42.38\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01553, Entropy loss 0.01105, Accuracy 92.12\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.00885, Entropy loss 0.01233, Accuracy 97.19\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.00780, Entropy loss 0.01373, Accuracy 97.88\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.00777, Entropy loss 0.01390, Accuracy 98.19\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.00790, Entropy loss 0.01353, Accuracy 98.56\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.00803, Entropy loss 0.01301, Accuracy 98.62\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.00810, Entropy loss 0.01253, Accuracy 98.81\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.00809, Entropy loss 0.01211, Accuracy 98.81\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.00801, Entropy loss 0.01176, Accuracy 98.94\n",
            "Epoch: 11\n",
            "\t Train: CE loss 0.00788, Entropy loss 0.01147, Accuracy 98.94\n",
            "Epoch: 12\n",
            "\t Train: CE loss 0.00775, Entropy loss 0.01122, Accuracy 99.12\n",
            "Epoch: 13\n",
            "\t Train: CE loss 0.00761, Entropy loss 0.01101, Accuracy 99.12\n",
            "Epoch: 14\n",
            "\t Train: CE loss 0.00748, Entropy loss 0.01082, Accuracy 99.12\n",
            "Epoch: 15\n",
            "\t Train: CE loss 0.00737, Entropy loss 0.01065, Accuracy 99.12\n",
            "baseline_accuracies: [77.   78.5  79.25 79.75 73.75]\n",
            "uda accuracies: [85.25 87.5  87.   87.   87.25]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Baseline mean: {baseline_accuracies.mean()} \\t Baseline std deviation: {baseline_accuracies.std()}\")\n",
        "print(f\"Uda mean: {uda_accuracies.mean()} \\t Uda std deviation: {uda_accuracies.std()}\")"
      ],
      "metadata": {
        "id": "wWO-P0HudRlt",
        "outputId": "937c06df-5dc3-48f2-c711-be9cbeed9592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Baseline mean: 77.65 \t Baseline std deviation: 2.1598611066455176\n",
            "Uda mean: 86.8 \t Uda std deviation: 0.7968688725254612\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ErFRn1RROV-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}