{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zinni98/DL-Project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bf-JVgEHsVSq"
      },
      "outputs": [],
      "source": [
        "from os import makedirs\n",
        "from os import listdir\n",
        "from os.path import join, isfile\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from shutil import copytree"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "metadata": {
        "id": "WVUMK-gw8w8V",
        "outputId": "2eb0eccf-76a0-47e6-a83d-23e5b3d44c65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Subset\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as F1\n",
        "\n",
        "\n",
        "def get_data(batch_size, root_dir):\n",
        "  \"\"\"\n",
        "\n",
        "  Params:\n",
        "  ------\n",
        "  root_dir: str\n",
        "    Directory of adaptiope_small (e.g. \"something/something_else/adaptiope_small\")\n",
        "  \"\"\"\n",
        "\n",
        "  # Transforms for resnet found there https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "  transform_img = list()\n",
        "  transform_img.append(T.Resize(256))\n",
        "  transform_img.append(T.CenterCrop(224))\n",
        "  transform_img.append(T.ToTensor())\n",
        "  transform_img.append(T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
        "  transform_img = T.Compose(transform_img)\n",
        "\n",
        "  # load data\n",
        "  product_images_dataset = ImageFolder(root = f\"{root_dir}/product_images/\", transform = transform_img)\n",
        "  rw_images_dataset = ImageFolder(root = f\"{root_dir}/real_life/\", transform = transform_img)\n",
        "\n",
        "  product_train_indexes, product_test_indexes = train_test_split(list(range(len(product_images_dataset.targets))),\n",
        "                                                test_size = 0.2, stratify = product_images_dataset.targets, random_state = 42)\n",
        "  \n",
        "  rw_train_indexes, rw_test_indexes = train_test_split(list(range(len(rw_images_dataset.targets))),\n",
        "                                                test_size = 0.2, stratify = rw_images_dataset.targets, random_state = 42)\n",
        "  \n",
        "\n",
        "  product_train_data = Subset(product_images_dataset, product_train_indexes)\n",
        "  product_test_data = Subset(product_images_dataset, product_test_indexes)\n",
        "\n",
        "  rw_train_data = Subset(rw_images_dataset, rw_train_indexes)\n",
        "  rw_test_data = Subset(rw_images_dataset, rw_test_indexes)\n",
        "\n",
        "  product_train_loader = torch.utils.data.DataLoader(product_train_data, batch_size, shuffle = False)\n",
        "  product_test_loader = torch.utils.data.DataLoader(product_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  rw_train_loader = torch.utils.data.DataLoader(rw_train_data, batch_size, shuffle = False)\n",
        "  rw_test_loader = torch.utils.data.DataLoader(rw_test_data, batch_size, shuffle = False)\n",
        "\n",
        "  return product_train_loader, product_test_loader, rw_train_loader, rw_test_loader\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wXlIpqRk8xZl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def initialize_resnet34(num_classes, pretrained = True):\n",
        "\n",
        "  model = torchvision.models.resnet34(pretrained=pretrained)\n",
        "\n",
        "  in_features = model.fc.in_features\n",
        "\n",
        "  ##model.fc = nn.Sequential(nn.Linear(512, num_classes))#, nn.LogSoftmax(dim = 1))\n",
        "  model.fc = nn.Linear(512, num_classes)\n",
        "  for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "Xq4k2RGV81M6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for labelled data\n",
        "def get_ce_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "metadata": {
        "id": "F-mp8CmM83cS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def get_optimizer_SGD(model, lr, wd, momentum):\n",
        "  \n",
        "  final_layer_weights = []\n",
        "  rest_of_the_net_weights = []\n",
        "  \n",
        "  for name, param in model.named_parameters():\n",
        "    if name.startswith('fc'): # ensure to work with parameters related to linear layer (7th)\n",
        "      final_layer_weights.append(param)\n",
        "    else:\n",
        "      rest_of_the_net_weights.append(param)\n",
        "\n",
        "  optimizer = torch.optim.SGD([\n",
        "      {'params': rest_of_the_net_weights},\n",
        "      {'params': final_layer_weights, 'lr': lr}\n",
        "  ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "  \n",
        "  return optimizer\n",
        "\n",
        "def get_optimizer_ADAM(net, lr = 0.0001, weight_decay = 0.000001):\n",
        "  opt = optim.Adam(params=net.parameters(), lr=lr, amsgrad=True, weight_decay = weight_decay, betas=(0.8, 0.9))\n",
        "  return opt"
      ],
      "metadata": {
        "id": "ZzIVcT7X85eg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_baseline(net, data_loader, optimizer, cost_function, device = 'cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  net.train() \n",
        " \n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "      \n",
        "    # forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # loss computation\n",
        "    loss = cost_function(outputs,targets)\n",
        "\n",
        "    # backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # parameters update\n",
        "    optimizer.step()\n",
        "\n",
        "    # gradients reset\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # fetch prediction and loss value\n",
        "    samples += inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(dim=1) # max() returns (maximum_value, index_of_maximum_value)\n",
        "\n",
        "    # compute training accuracy\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, (cumulative_accuracy/samples)*100\n",
        "\n",
        "\n",
        "\n",
        "def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  # set the network to evaluation mode\n",
        "  net.eval() \n",
        "\n",
        "  # disable gradient computation (we are only testing, we do not want our model to be modified in this step!)\n",
        "  with torch.no_grad():\n",
        "\n",
        "    # iterate over the test set\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      \n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "        \n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # loss computation\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # fetch prediction and loss value\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "\n",
        "      # compute accuracy\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "DMiSWtTz87wG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def main(batch_size=128, \n",
        "         device='cuda', \n",
        "         learning_rate=0.0001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = 20, \n",
        "         img_root=\"gdrive/My Drive/Colab Notebooks/data/adaptiope_small\"\n",
        "         ):\n",
        "\n",
        "  writer = SummaryWriter(log_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\")\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = initialize_resnet34(nr_classes).to(device)\n",
        "  print('Network Init Done')\n",
        "  #optimizer = get_optimizer_SGD(net, learning_rate, wd = weight_decay, momentum = momentum)\n",
        "  optimizer = get_optimizer_ADAM(net, learning_rate, weight_decay)\n",
        "  print('Got Optimizer')\n",
        "  cost_function = get_ce_cost_function()\n",
        "  print('Got Cost Function')\n",
        "  print('Time to train!\\n==========================BASELINE========================')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    ##BASELINE\n",
        "\n",
        "\n",
        "    # def training_step_baseline(net, data_loader, optimizer, cost_function, scheduler, device='cuda'):\n",
        "    train_loss, train_accuracy = training_step_baseline(net, source_train_loader, optimizer, cost_function, device)\n",
        "    #def test_step_baseline(net, data_loader, cost_function, device='cuda'):\n",
        "    test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # add values to logger\n",
        "    writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "  \n",
        "\n",
        "  # perform final test step and print the final metrics\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test_step_baseline(net, source_train_loader, cost_function, device)\n",
        "  test_loss, test_accuracy = test_step_baseline(net, target_test_loader, cost_function, device)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "\n",
        "  \n",
        "  # close the logger\n",
        "  writer.close()"
      ],
      "metadata": {
        "id": "JDFkQoLq8-oD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SymNet(nn.Module):\n",
        "  def __init__(self, n_classes: int = 20):\n",
        "    super(SymNet, self).__init__()\n",
        "    # Taking the feature extractor of resnet34\n",
        "    # Reference: https://stackoverflow.com/questions/55083642/extract-features-from-last-hidden-layer-pytorch-resnet18\n",
        "    resnet = initialize_resnet34(20, True)\n",
        "    self.feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])\n",
        "    # print(self.feature_extractor)\n",
        "    self.source_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "    self.target_classifier = nn.Linear(in_features=512, out_features=n_classes)\n",
        "  \n",
        "\n",
        "  def forward(self, x):\n",
        "    features = self.feature_extractor(x)\n",
        "    features = features.squeeze()\n",
        "    source_output = self.source_classifier(features)\n",
        "    # source_output = nn.Softmax(source_output)\n",
        "\n",
        "    target_output = self.target_classifier(features)\n",
        "    # target_output = nn.Softmax(target_output)\n",
        "\n",
        "    source_target_classifier = torch.cat((source_output, target_output), dim=1)\n",
        "    \n",
        "    return source_output , target_output, source_target_classifier\n",
        "  \n",
        "  def parameters(self):\n",
        "    fe = list(self.feature_extractor.parameters())\n",
        "    sc = list(self.source_classifier.parameters())\n",
        "    tc = list(self.target_classifier.parameters())\n",
        "    tot = fe + sc + tc\n",
        "    for param in tot:\n",
        "      yield param\n"
      ],
      "metadata": {
        "id": "jw9TzdJA8--0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_optimizer_SGD_uda(model, lr, wd, momentum, e, nr_epochs, classifier = True):\n",
        "  #if not e==0:\n",
        "  eta0 = 0.01\n",
        "  alpha = 10\n",
        "  beta = 0.75\n",
        "  etap = eta0 / ((1 + alpha * e / nr_epochs ) ** beta)\n",
        "  lr = etap * lr\n",
        "  if classifier: # because Cst has a lr 10 times bigger than G\n",
        "    lr = lr * 10\n",
        "    params = list(model.source_classifier.parameters()) + list(model.target_classifier.parameters())\n",
        "    optimizer = torch.optim.SGD(params, lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  else:\n",
        "    optimizer = torch.optim.SGD(model.feature_extractor.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer\n",
        "\n",
        "def get_optimizer_ADAM_uda(model, e, nr_epochs, lr=0.0001, wd = .000001):\n",
        "  eta0 = 0.01\n",
        "  alpha = 10\n",
        "  beta = 0.75\n",
        "  etap = eta0 / ((1 + alpha * e / nr_epochs ) ** beta)\n",
        "  lr = etap * lr\n",
        "  optimizer = optim.Adam(model.parameters(), lr=lr, amsgrad=True, weight_decay = wd, betas=(0.8, 0.9))\n",
        "  \"\"\"if classifier: # because Cst has a lr 10 times bigger than G\n",
        "    lr = lr * 10\n",
        "    params = list(model.source_classifier.parameters()) + list(model.target_classifier.parameters())\n",
        "    optimizer = optim.Adam(params, lr=lr, amsgrad=True, weight_decay = wd, betas=(0.8, 0.9))\n",
        "  else:\n",
        "    optimizer = optim.Adam(params=model.feature_extractor.parameters(), lr=lr, amsgrad=True, weight_decay = wd, betas=(0.8, 0.9))\"\"\"\n",
        "  return optimizer\n",
        "  "
      ],
      "metadata": {
        "id": "ARWQy0sv8_at"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def source_loss(output, label):\n",
        "  \"\"\"\n",
        "  Returns\n",
        "  -------\n",
        "  Cross entropy loss\n",
        "  \"\"\"\n",
        "  loss_fun = nn.CrossEntropyLoss()\n",
        "  loss = loss_fun(output, label)\n",
        "  return loss\n",
        "\n",
        "def target_loss(output, label):\n",
        "  return source_loss(output, label)\n",
        "\n",
        "def source_target_loss(output, st = True):\n",
        "  \"\"\"\n",
        "  st = True if train sample belongs to source, False otherwise\n",
        "  \"\"\"\n",
        "  n_classes = int(output.size(1)/2)\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output)\n",
        "  if st:\n",
        "    loss = -(prob_out[:,:n_classes].sum(1).log().mean())\n",
        "  else:\n",
        "    loss = -(prob_out[:,n_classes:].sum(1).log().mean())\n",
        "  return loss\n",
        "\n",
        "def feature_category_loss(output_st, label):\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  loss_fun_1 = nn.CrossEntropyLoss()\n",
        "  loss_fun_2 = nn.CrossEntropyLoss()\n",
        "\n",
        "  loss_1 = loss_fun_1(output_st[:, :n_classes], label)/2\n",
        "  loss_2 = loss_fun_2(output_st[:,n_classes:], label)/2\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "def feature_domain_loss(output_st):\n",
        "  n_classes = int(output_st.size(1)/2)\n",
        "\n",
        "  soft = nn.Softmax(dim=1)\n",
        "  prob_out = soft(output_st)\n",
        "\n",
        "  loss_1 = -(prob_out[:,:n_classes]).sum(1).log().mean()/2\n",
        "  loss_2 = -(prob_out[:,n_classes:]).sum(1).log().mean()/2\n",
        "\n",
        "  return loss_1 + loss_2\n",
        "\n",
        "\n",
        "\n",
        "def entropyMinimizationPrinciple(output_st):\n",
        "    nr_classes = int(output_st.size(1)/2)\n",
        "    soft = nn.Softmax(dim=1)\n",
        "    prob_out = soft(output_st)\n",
        "\n",
        "    p_st_source = prob_out[:, :nr_classes]\n",
        "    p_st_target = prob_out[:, nr_classes:]\n",
        "    qst = p_st_source + p_st_target\n",
        "\n",
        "    emp = -qst.log().mul(qst).sum(1).mean()\n",
        "\n",
        "    return emp"
      ],
      "metadata": {
        "id": "cSa0gFgR9HEC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def training_step_uda(net, src_data_loader, target_data_loader, optimizer_1, lam, e,device = 'cuda'):\n",
        "  source_samples = 0.\n",
        "  target_samples = 0.\n",
        "  cumulative_classifier_loss = 0.\n",
        "  cumulative_feature_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  target_iter = iter(target_data_loader)\n",
        "\n",
        "  net.train()\n",
        "\n",
        "  # iterate over the training set\n",
        "  for batch_idx, (inputs_source, labels) in enumerate(src_data_loader):\n",
        "    try:\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    except:\n",
        "      target_iter = iter(target_data_loader)\n",
        "      inputs_target, _ = next(target_iter)\n",
        "      inputs_target = inputs_target.to(device)\n",
        "    \n",
        "    # load data into GPU\n",
        "    inputs_source = inputs_source.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    print(f\"Batch number: {batch_idx}\")\n",
        "\n",
        "    length_source_input = inputs_source.shape[0]\n",
        "\n",
        "    ## concatenation along batch dimension.\n",
        "    inputs = torch.cat((inputs_source, inputs_target), dim=0)\n",
        "\n",
        "    # forward pass\n",
        "    c_s, c_t, c_st = net(inputs)\n",
        "\n",
        "    c_s_source = c_s[:length_source_input,:]\n",
        "    c_s_target = c_s[length_source_input:,:]\n",
        "\n",
        "    c_t_source = c_t[:length_source_input,:]\n",
        "    c_t_target = c_t[length_source_input:,:]\n",
        "\n",
        "    c_st_source = c_st[:length_source_input,:]\n",
        "    c_st_target = c_st[length_source_input:,:]\n",
        "\n",
        "\n",
        "    # Equation 5 of the paper\n",
        "    error_source_task = source_loss(c_s_source, labels)\n",
        "\n",
        "    # Equation 6 of the paper\n",
        "    error_target_task = target_loss(c_t_source, labels)\n",
        "\n",
        "    # Equation 7 of the paper\n",
        "    domain_loss_source = source_target_loss(c_st_source)\n",
        "    domain_loss_target = source_target_loss(c_st_target, st = False)\n",
        "    error_domain = domain_loss_source + domain_loss_target\n",
        "\n",
        "    classifier_total_loss = error_source_task + error_target_task + error_domain\n",
        "\n",
        "    classifier_total_loss.backward(retain_graph = True)\n",
        "\n",
        "    for param in net.feature_extractor.parameters():\n",
        "      param.grad.data.zero_()\n",
        "    \n",
        "    class_params = []\n",
        "    for param in net.source_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "    for param in net.target_classifier.parameters():\n",
        "      class_params.append(param.grad.data.clone())\n",
        "      param.grad.data.zero_()\n",
        "\n",
        "    # Equation 8 of the paper\n",
        "    error_feature_category = feature_category_loss(c_st_source, labels)\n",
        "\n",
        "    # Equation 9 of the paper\n",
        "    error_feature_domain = feature_domain_loss(c_st_target)\n",
        "\n",
        "    min_entropy = entropyMinimizationPrinciple(c_st_target)\n",
        "\n",
        "    # Equations 11 of the paper\n",
        "    feature_total_loss = error_feature_category + lam * (error_feature_domain + min_entropy)\n",
        "\n",
        "    feature_total_loss.backward()\n",
        "\n",
        "    idx = 0\n",
        "    for param in net.source_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "    for param in net.target_classifier.parameters():\n",
        "      param.grad.data = class_params[idx]\n",
        "      idx += 1\n",
        "\n",
        "    \n",
        "    optimizer_1.step()\n",
        "    optimizer_1.zero_grad()\n",
        "\n",
        "    ## optimizer classifier losses composed loss\n",
        "    ## order is important here!\n",
        "    \n",
        "\n",
        "\n",
        "    # print statistics\n",
        "    source_samples+=inputs_source.shape[0]\n",
        "    target_samples+=inputs_target.shape[0]\n",
        "    \n",
        "    cumulative_classifier_loss += classifier_total_loss.item()\n",
        "    cumulative_feature_loss += feature_total_loss.item()\n",
        "    _, predicted = c_s_source.max(dim = 1) ## to get the maximum probability\n",
        "    cumulative_accuracy += predicted.eq(labels).sum().item()\n",
        "\n",
        "  return cumulative_classifier_loss/source_samples, cumulative_feature_loss/target_samples, cumulative_accuracy/source_samples*100\n"
      ],
      "metadata": {
        "id": "dGdsgnes9HcG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(net, data_target_test_loader, device='cuda:0'):\n",
        "\n",
        "    '''\n",
        "    Params\n",
        "    ------\n",
        "\n",
        "    net : model \n",
        "    data_loader : DataLoader obj of the domain to test on\n",
        "    cost_function : cost function used to address accuracies (not necessary) -> TargetClassifierLoss\n",
        "    device : GPU or CPU device\n",
        "\n",
        "    '''\n",
        "\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch_idx, (inputs, labels) in enumerate(data_target_test_loader):\n",
        "\n",
        "            # load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = labels.to(device)\n",
        "        \n",
        "            # forward pass\n",
        "            _, c_t, _ = net(inputs)\n",
        "\n",
        "            # apply the loss\n",
        "            loss = target_loss(c_t, targets)\n",
        "\n",
        "            # print statistics\n",
        "            samples+=inputs.shape[0]\n",
        "            cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "            _, predicted = c_t.max(1)\n",
        "            cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "metadata": {
        "id": "hQiFy0TZ9j3b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['backpack', 'bookcase', 'car jack', 'comb', 'crown', 'file cabinet', 'flat iron', 'game controller', 'glasses', 'helicopter', 'ice skates', 'letter tray', 'monitor', 'mug', 'network switch', 'over-ear headphones', 'pen', 'purse', 'stand mixer', 'stroller']\n",
        "\n",
        "cuda = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 128\n",
        "num_classes = len(classes)\n",
        "rootdir_matteo = '/content/gdrive/MyDrive/Colab Notebooks/Deep Learning labs/DA Project/adaptiope_small'\n",
        "rootdir_alessandro = 'gdrive/My Drive/Colab Notebooks/data/adaptiope_small'\n",
        "rootdir_alessandro_uni = 'gdrive/My Drive/project/data/adaptiope_small'"
      ],
      "metadata": {
        "id": "ixVFg1jb9mLU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import math\n",
        "\n",
        "def main_uda(batch_size=128,\n",
        "         device=cuda, \n",
        "         lr = 0.01,\n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=10,\n",
        "         entropy_loss_weight=0.1,\n",
        "         nr_classes = num_classes, \n",
        "         img_root=rootdir_alessandro_uni\n",
        "         ):\n",
        "    \n",
        "  # writer = SummaryWriter(log_dir=\"gdrive/My Drive/Colab Notebooks/runs/exp2\")\n",
        "\n",
        "  ## DataLoader split the size of the given dataset into #of elements in the dataset/batch size\n",
        "  source_train_loader, source_test_loader, target_train_loader, target_test_loader = get_data(batch_size, img_root)\n",
        "  print('DataLoaders Done')\n",
        "  net = SymNet().to(device)\n",
        "  print('Network Init Done')\n",
        "  optimizer_1 = get_optimizer_ADAM_uda(model=net, e=0, nr_epochs = epochs,lr=lr, wd=weight_decay)\n",
        "  # optimizer_2 = get_optimizer_ADAM_uda(model=net, lr=lr, wd=weight_decay, e=0, nr_epochs=epochs, classifier=False)\n",
        "  print('Got optimizers')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    lam = 2 / (1 + math.exp(-1 * 10 * e / epochs)) - 1\n",
        "    #def training_step_uda(net, src_data_loader, target_data_loader, optimizer_1, optimizer_2, lam, device = 'cuda')\n",
        "    train_ce_loss, train_en_loss, train_accuracy = training_step_uda(net=net, src_data_loader=source_train_loader, \n",
        "                                                        target_data_loader=target_train_loader, \n",
        "                                                        optimizer_1=optimizer_1, lam=lam, e=e, device=device)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    test_loss, test_accuracy = test_step(net, target_test_loader, device)\n",
        "\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Train: CE loss {:.5f}, Entropy loss {:.5f}, Accuracy {:.2f}'.format(train_ce_loss, train_en_loss, train_accuracy))\n",
        "    print('\\t Test: CE loss {:.5f}, Accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')  "
      ],
      "metadata": {
        "id": "h2-rMG7B9u6s"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_uda()"
      ],
      "metadata": {
        "id": "sLOVe5Q99xmI",
        "outputId": "4c4a3880-35ec-4c9a-968d-30fc55e0ffc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders Done\n",
            "Network Init Done\n",
            "Got optimizers\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 1\n",
            "\t Train: CE loss 0.03990, Entropy loss 0.01437, Accuracy 55.38\n",
            "\t Test: CE loss 0.01664, Accuracy 68.25\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 2\n",
            "\t Train: CE loss 0.01472, Entropy loss 0.01405, Accuracy 97.69\n",
            "\t Test: CE loss 0.01076, Accuracy 77.25\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 3\n",
            "\t Train: CE loss 0.01323, Entropy loss 0.01346, Accuracy 99.50\n",
            "\t Test: CE loss 0.00680, Accuracy 80.75\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 4\n",
            "\t Train: CE loss 0.01252, Entropy loss 0.00929, Accuracy 99.88\n",
            "\t Test: CE loss 0.00699, Accuracy 81.25\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 5\n",
            "\t Train: CE loss 0.01214, Entropy loss 0.00710, Accuracy 100.00\n",
            "\t Test: CE loss 0.00764, Accuracy 79.75\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 6\n",
            "\t Train: CE loss 0.01192, Entropy loss 0.00637, Accuracy 100.00\n",
            "\t Test: CE loss 0.00671, Accuracy 84.00\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 7\n",
            "\t Train: CE loss 0.01179, Entropy loss 0.00613, Accuracy 100.00\n",
            "\t Test: CE loss 0.00715, Accuracy 83.25\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 8\n",
            "\t Train: CE loss 0.01177, Entropy loss 0.00605, Accuracy 100.00\n",
            "\t Test: CE loss 0.00728, Accuracy 83.50\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 9\n",
            "\t Train: CE loss 0.01178, Entropy loss 0.00601, Accuracy 100.00\n",
            "\t Test: CE loss 0.00737, Accuracy 83.50\n",
            "-----------------------------------------------------\n",
            "Batch number: 0\n",
            "Batch number: 1\n",
            "Batch number: 2\n",
            "Batch number: 3\n",
            "Batch number: 4\n",
            "Batch number: 5\n",
            "Batch number: 6\n",
            "Batch number: 7\n",
            "Batch number: 8\n",
            "Batch number: 9\n",
            "Batch number: 10\n",
            "Batch number: 11\n",
            "Batch number: 12\n",
            "Epoch: 10\n",
            "\t Train: CE loss 0.01179, Entropy loss 0.00598, Accuracy 100.00\n",
            "\t Test: CE loss 0.00748, Accuracy 83.25\n",
            "-----------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zHLj71n5aXZp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}